## RNN & LSTM
## Recurrent Neural Networks (RNN)
- **What is RNN?**  
  RNN carries previous information and allow them to persist. It use loops in the networks to retain the information.

  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" width="200px" /><br>
  <strong>Figure 1, RNN with loops</strong>
  </div>

  Network block A, input $x_i$ and output $h_t$, The loop can carry the information from one step to next.

<!--more-->

  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="600px" /><br>
  <strong>Figure 2, An unrolled recurrent neural network</strong>
  </div>

  The chain-like net  is vary suitable for sequence data. It usually used to construct language modeling, speech recognition and video feature extracting..., some RL models using RNN + LSTM to extract the latent information from several continuous video frames as the model inputs. In general, RNN uses loops to merge features of related data and passes these features as hidden states to next step to keep information in the networks. It allows the model to use the previous information to archive the present task, such as inferring the last word for a sentence by language model, understanding the present video frame by previous video frames.  
- Limitation
  If the network-chains is long, the network can't keep initial information. The network is myopia.
<br>

## Long Short Term Memory networks (LSTM)
introduced by [Hochreiter & Schmidhuber](http://www.bioinf.jku.at/publications/older/2604.pdf) 1997 
- **What is LSTM ?**  
  LSTM is a special kind of RNN, having ability of learning lean-term dependencies. It is exactly designed to overcome the long-term dependency problem.
- **Structure**  
  A standard RNN module have very simple structure, containing a single tanh layer. 
  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" width="700px" /><br>
  <strong>Figure 3, The repeating module in a standard RNN contains a single layer</strong>
  </div>
  LSTM also have chain like structure, but the module is more complex than standard RNN
  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="700px" /><br>
  <strong>Figure 4, The repeating module in an LSTM contains four interacting layers</strong>
  </div>

- **Idea**  
  Cell state $C_t$ run thought the top of the diagram. It like a conveyor belt, carrying information straight down the entire chain.  
  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" width=700px /><br>
  <strong>Figure 5, Cell State</strong>
  </div>

  Gate have the capable of removing or adding information to the cell state. It optionally let information go or block by a sigmoid layer and a element-wise multiplication operation. 
  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" width="100px" />
  <br>
  <strong>Figure 6, Gate</strong>
  </div>
  An LSTM module has three gates to protect and control the cell state.  
  <br/><br/>
  The sigmoid layer decide what information we're going to throw away from the cell state
  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" width="700px" />
  <br>
  <strong>Figure 7, Forget gate layer</strong>
  </div>
  This layer decide what new information we're going to store in the cell state. 

  - Sigmoid: decides which values we'll update 
  - tanh: creates a vector of new candidate values, $\hat{C}_t$, could be added to the state. 
  
  Next, combine these two  to create an update to the state. 
  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" width="700px" />
  <br>
  <strong>Figure 8, Input gate layer</strong>
  </div>

  Update the old cell state, $C_{t-1}$ into the new cell state $C_t$.  

  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" width="700px" />
  <br>
  <strong>Figure 9, Update cell state</strong>
  </div>
  
  Output:
  - Sigmoid layer decides what parts of the cell state are going to output
  - Cell state through $\tanh$ and multiply it by the output of the sigmoid gate

  <div align="center">
  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" width="700px" />
  <br>
  <strong>Figure 10, Output hidden state</strong>
  </div>


### Reference:
colah's blog [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
Hochreiter & Schmidhuber, [LONG SHORT-TERM MEMORY](http://www.bioinf.jku.at/publications/older/2604.pdf), 1997