<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.55.6" />

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">

<link rel="alternate" type="application/rss&#43;xml" href="/docs/index.xml">

<link rel="shortcut icon" href="/assets/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/assets/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/assets/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/assets/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/assets/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/assets/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/assets/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/assets/favicons/android-96x196.png" sizes="96x196">
<link rel="icon" type="image/png" href="/assets/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/assets/favicons/android-192x192.png"sizes="192x192">

<title>Function approximation</title>
<meta property="og:title" content="Function approximation" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:4000" />
<meta property="og:site_name" content="http://localhost:4000" />

<meta itemprop="name" content="Function approximation">
<meta itemprop="description" content="">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Function approximation"/>
<meta name="twitter:description" content=""/>

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="/assets/css/palette.css">
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>
</head>

  

  <body class="td-section">
    <header>
        <nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
            <span class="navbar-logo"></span><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149v0 0zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.328-2.2733-15.458-6.4032s-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zM197.0804 232.033c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zM197.0839 232.0372c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zM197.0839 232.0372c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><path style="fill:#5b7fc0" d="M198.8952 225.1043h122.6266v13.8671H198.8952z"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.328-2.2733-15.458-6.4032s-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zM197.0804 177.6188c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zM197.0839 177.623c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zM197.0839 177.623c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><path style="fill:#d95140" d="M198.8952 170.69h122.6266v13.8671H198.8952z"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zM197.5309 286.4723c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zM197.5344 286.4765c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zM197.5344 286.4765c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><path style="fill:#56a55c" d="M199.3456 279.5436h122.6266v13.8671H199.3456z"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.328-2.2733-15.458-6.4032-4.13-4.1299-6.4032-9.6186-6.4056-15.4628.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zM197.0804 340.5784c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zM197.0839 340.5826c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zM197.0839 340.5826c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><path style="fill:#f1bc42" d="M198.8952 333.6497h122.6266v13.8671H198.8952z"/></g></g></svg>
<span class="text-uppercase font-weight-bold">Yukai Wu</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			<li class="nav-item mr-4 mb-2 mb-lg-0">
                            <a class="nav-link" href="/about" ><span>About</span></a>
			</li>
			<li class="nav-item mr-4 mb-2 mb-lg-0">
                            <a class="nav-link" href="/about" ><span>Documentation</span></a>
			</li>
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block">
 <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
        </div>

	<div class="navbar-nav d-none d-lg-block">
          <a class="gh-source" data-gh-source="github" href="" title="Go to repository" data-md-state="done">
          <div class="gh-source__repository">
            <i class="fab fa fa-github fa-2x" style='padding-right:20px; float:left; margin-top:5px'></i>
            yukai wu/yukai wu
          <ul class="gh-source__facts"><li class="gh-source__fact" id='stars'></li><li id="forks" class="gh-source__fact"></li></ul></div></a>
        </div>
      </div>


</nav>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                            });
  </script>
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</header>


<script>
$(document).ready(function() {
  var url = "https://api.github.com/search/repositories?q=yukai wu/yukai wu";
  fetch(url, { 
      headers: {"Accept":"application/vnd.github.preview"}
  }).then(function(e) {
    return e.json()
  }).then(function(r) {
     console.log(r.items[0])
     stars = r.items[0]['stargazers_count']
     forks = r.items[0]['forks_count']
     $('#stars').text(stars + " Stars")
     $('#forks').text(forks + " Forks")
  });
});
</script>

    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          <div id="td-sidebar-menu" class="td-sidebar__inner">  
  <form class="td-sidebar__search d-flex align-items-center">
 <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>  
  <nav class="collapse td-sidebar-nav pt-2 pl-4" id="td-section-nav">
<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/about" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">About Me</a>
  </li><ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/news" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">Study Note</a>
  </li>
  </nav>
</div>

          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
              <div class="td-page-meta ml-2 pb-1 pt-2 mb-0">
                  <a href="/edit/master/_posts/2019-05-12-Function Approximation.md" target="_blank"><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="/issues/new?labels=question&title=Question:&body=Question on: /tree/master/_posts/2019-05-12-Function Approximation.md" target="_blank"><i class="fab fa-github fa-fw"></i> Create documentation issue</a>
<a href="/issues/new" target="_blank"><i class="fas fa-tasks fa-fw"></i> Create project issue</a>
<!-- this will parse through the header fields and add a button to open
     an issue / ask a question on Github. The editable field should be in
     the post frontend matter, and refer to the label to open the issue for -->

              </div>
              <nav id="TableOfContents"><ul>
              <li><ul id="TOC">
                <!-- Links will be appended here-->
              </ul></li>
              </ul></nav>
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            <nav aria-label="breadcrumb" class="d-none d-md-block d-print-none">
	      <ol class="breadcrumb spb-1">
                <li class="breadcrumb-item active" aria-current="page">
	          <a href="/2019/Function-Approximation/">Function approximation</a>
                </li>
	      </ol>
           </nav>
           <div class="td-content">
	      <h1 style="margin-bottom:0px">Function approximation</h1>

<span class="post-date" style="font-style: italic;">May 12, 2019</span>
<h3 id="introduce">Introduce</h3>
<hr />
<ul>
  <li>The <strong>policy</strong>, <strong>value function</strong> and <strong>model</strong> are all functions</li>
  <li>We want to learn (one of) these from experience</li>
  <li>If there are too many states, we need to approximate</li>
  <li>In general, this is called RL with function approximation</li>
  <li>When using deep neural nets, this is often called deep reinforcement learning</li>
  <li>The term is fairly new — the combination is decades old</li>
</ul>

<!--more-->
<h3 id="value-function-approximation">Value Function Approximation</h3>
<hr />
<ul>
  <li><strong>lookup tables</strong>
    <ul>
      <li>Every state $s$ has an entry $q(s,a)$</li>
      <li>Or every state-action pair $s,a$ has an entry $q(s,a)$</li>
    </ul>
  </li>
  <li>Large MDPs:
    <ul>
      <li>There are too many states and/or actions to store in memory</li>
      <li>It is too slow to learn the value of each state individually</li>
      <li>Individual states are often <strong>not fully observable</strong></li>
    </ul>
  </li>
  <li>Solution for large MDPs:
    <ul>
      <li>Estimate value function with <strong>function approximation</strong><br />
<script type="math/tex">% <![CDATA[
\begin{aligned}
v_\theta(s)\approx v_\pi(s) \qquad &(or, v_*(s)) \\
q_\theta(s,a)\approx q_\pi(s,a) \qquad &(or,q_*(s,a))
\end{aligned} %]]></script></li>
      <li><strong>Generalise</strong> from seen states to unseen states</li>
      <li><strong>Update</strong> parameter $\theta$ using MC or TD observable</li>
    </ul>
  </li>
  <li>If the environement state is not fully observable:
    <ul>
      <li>Use the <strong>agent state</strong></li>
      <li>Consider learning a <strong>state update function</strong> $S_{t+1}=u(S_t,O_{t+1})$</li>
      <li>Henceforth, $S_t$ denotes the agent state</li>
    </ul>
  </li>
</ul>

<h3 id="which-function-approximator">Which Function Approximator?</h3>
<p>There are many function approximators, e.g.</p>
<ul>
  <li>Artificial neural network</li>
  <li>Decision tree</li>
  <li>Nearest neighbour</li>
  <li>Fourier / wavelet bases</li>
  <li>Coarse coding</li>
</ul>

<p>In principle, <strong>any</strong> function approximator can be used, but RL has specific properties:</p>
<ul>
  <li>Experience is not i.i.d — successive time-step are correlated</li>
  <li>Agent’s policy affects the data it receives</li>
  <li>Value functions $v_\pi(s)$ can be non-stationary</li>
  <li>Feedback is delayed, not instantaneous</li>
</ul>

<h3 id="classes-of-function-approximation">Classes of Function Approximation</h3>
<hr />
<ul>
  <li>Tabular: a table with an entry for each MDP state</li>
  <li>State aggregation: Partition environment states</li>
  <li>Linear function approximate: fixed feature (or fixed kernel)</li>
  <li>Differentiable (nonlinear) function approximation: neural nets</li>
</ul>

<h3 id="approximate-values-by-stochastic-gradient-descent">Approximate Values By Stochastic Gradient Descent</h3>
<hr />
<ul>
  <li>Goal: fins $\theta$ that minimise the difference between $v_\theta(s)$ and   $v_\pi(s)$ 
<script type="math/tex">J(\theta)=\mathbb{E}[(v_\pi(S)-v_\theta(S))^2]</script>
      Note: The expectation if over the state distribution — e.g., induced by the policy</li>
  <li>Gradient descent:<br />
<script type="math/tex">\Delta\theta=-\frac{1}{2}\alpha\nabla_\theta J(\theta)=\alpha\mathbb{E}[(v_\pi(S)-v_\theta(S))\nabla_\theta v_\theta(S)]</script></li>
  <li><strong>Stochastic</strong> gradient descent<br />
<script type="math/tex">\Delta\theta_t=\alpha(v_\pi(S_t)-v_\theta(S_t))\nabla_\theta v_\theta(S_t)</script>
    <h3 id="feature-vectors">Feature Vectors</h3>
    <hr />
  </li>
  <li>Represent state by a <strong>feature vector</strong> <br />
<script type="math/tex">\phi(s) = \left(\begin{array}{cc}
\phi_1(s) \\ \vdots \\ \phi_n(s)
\end{array}\right)</script></li>
  <li>$\phi:S\rightarrow\mathbb{R}^n$ is a fixed mapping from state (e.g. observation) to features</li>
  <li>Short-hand: $\phi_t=\phi(S_t)$</li>
  <li>For example:
    <ul>
      <li>Distance of robot from landmarks</li>
      <li>Trends in the stock market</li>
      <li>Piece and pawn configurations in chess</li>
    </ul>
  </li>
</ul>

<h3 id="approximate-values-by-stochastic-gradient-descent-1">Approximate Values By Stochastic Gradient Descent</h3>
<ul>
  <li>Goal: fina $\theta$ that minimise the difference between $v_\theta(s)$ and $v_\pi(s)$ 
<script type="math/tex">J(\theta)=\mathbb{E}[(v_\pi(S)-v_\theta(S))^2]</script>  <br />
Note: The expectation if over the state distribution — e.g., induced by the policy.</li>
  <li>Gradient descent:<br />
<script type="math/tex">\Delta\theta=-\frac{1}{2}\alpha\Delta_\theta J(\theta)=\alpha\mathbb{E}_\pi p[(v_\pi(S)-v_\theta(S))\nabla_\theta v_\theta(S)]</script></li>
  <li><strong>Stochastic</strong> gradient descent:<br />
<script type="math/tex">\Delta\theta_t=\alpha(v_\pi(S_t)-v_\theta(S_t))\nabla_\theta v_\theta(S_t)</script></li>
</ul>

<h3 id="linear-value-function-approximation">Linear Value Function Approximation</h3>
<ul>
  <li>Approximate value function by a linear combination of features<br />
<script type="math/tex">v_\theta(s)=\theta^\top \phi(s)=\sum_{j=1}^n\phi_j(s)\theta_j</script></li>
  <li>Objective function (‘loss’) is quadratic in $\theta$<br />
<script type="math/tex">J(\theta)=\mathbb{E}_\pi\big[(v_\pi(S)-\theta^\top\phi(S))^2\big]</script></li>
  <li>Stichastic gradient descent converges on global ooptimum</li>
  <li>Update rule is simple<br />
<script type="math/tex">\nabla_\theta v_\theta(S_t)=\phi(S_t)=\phi_t \quad \Longrightarrow \quad \Delta_\theta = \alpha(v_\pi(S_t)-v_\theta(S_t))\phi_t</script>
      $\large \text{Update}=\textbf{step size}\times\textbf{prediction error}\times\textbf{feature vector}$</li>
</ul>

<h3 id="incremental-prediction-algorithms">Incremental Prediction Algorithms</h3>
<hr />
<ul>
  <li>The true value function $v_\pi(s)$ is typically not available</li>
  <li>In practice, we substitute a <strong>target</strong> for $v_\pi(s)$
    <ul>
      <li>For MC, the target is the return $G_t$<br />
<script type="math/tex">\Delta\theta_t=\alpha(G_t - v_\theta(s))\nabla_\theta v_\theta(s)</script></li>
      <li>For TD, the target is the TD target $R_{t+1}+\gamma v_\theta(S_{t+1})$<br />
<script type="math/tex">\Delta\theta_t=\alpha(R_{t+1}+\gamma v_\theta(S_{t+1}) - v_\theta(S_t))\nabla_\theta v_\theta(S_t)</script></li>
    </ul>
  </li>
</ul>

<h3 id="monte-carlo-with-value-function-approximation">Monte-Carlo with Value Function Approximation</h3>
<hr />
<ul>
  <li>The return $G_t$ is an unbiased, noisy sample of $v_\pi(s)$</li>
  <li>Can therefore apply supervised learning to (online) “training data”<br />
<script type="math/tex">{(S_0,G_0),\ldots,(S_t,G_t)}</script></li>
  <li>For example, using <strong>linear Monte-Carlo policy evaluation</strong><br />
<script type="math/tex">% <![CDATA[
\begin{aligned}
\Delta\theta_t &=\alpha(G_t - v_\theta(S_t))\nabla_\theta v_\theta(S_t) \\
& = \alpha(G_t - v_\theta(S_t))\phi_t
\end{aligned} %]]></script></li>
  <li>Monte-Carlo evaluation converges to a local optimum</li>
  <li>Even when using non-linear value function approximation</li>
  <li>For linear function, it finds the globlal optimum</li>
</ul>

<h3 id="td-learning-with-value-function-approximation">TD Learning with Value Function Approximation</h3>
<hr />
<ul>
  <li>The TD-target $R_{t+1}+\gamma v_\theta(S_{t+1})$ is a <strong>biased</strong> sample og true value $v_\pi(S_t)$</li>
  <li>Can still apply supervised learning to “training data”<br />
<script type="math/tex">{(S_0,R_1+\gamma v_\theta(S_1)),\ldots,(S_t,R_{t+1}+\gamma v_\theta(S_{t+1}))}</script></li>
  <li>For example, using linear TD<br />
<script type="math/tex">% <![CDATA[
\begin{aligned}
\Delta\theta_t &= \alpha\underbrace{(R_{t+1}+\gamma v_\theta(S_{t+1})-v_\theta(S_t))}_{\normalsize =\delta_t, \text{TD error}}\nabla_\theta v_\theta(S_t) \\
& =\alpha\delta_t\phi_t
\end{aligned} %]]></script></li>
</ul>

<h3 id="convergence-of-mc-and-td">Convergence of MC and TD</h3>
<hr />
<ul>
  <li>
    <p>with linear functions, MC converges to<br />
<script type="math/tex">\min_\theta\mathbb{E}\big[(G_t-v_\theta(S_t))^2\big]=\mathbb{E}\big[\phi_t\phi_t^\top\big]^{-1}\mathbb{E}\big[v_\pi(S_t)\phi_t\big]</script></p>
  </li>
  <li>With linear function, TD converges to<br />
<script type="math/tex">\min_\theta\mathbb{E}\big[(R_{t+1}+\gamma v_\theta(S_{t+1}-v_\theta(S_t)))^2\big]=\mathbb{E}\big[\phi_t(\phi_t-\gamma\phi_{t+1})^\top\big]\mathbb{E}\big[R_{t+1}\phi_t\big]</script>
(in continuing problem with fixed $\gamma$)</li>
  <li>This is a different solution from MC</li>
  <li>Typically, the asymptotic MC solution is preferred</li>
  <li>But TD methods may converge faster,   and may still be better<br />
<script type="math/tex">\textbf{TD:}\quad\Delta_t=\alpha\delta\nabla_\theta v_\theta(S_t)\quad \textbf{where} \quad \delta_t=R_{t+1}+\gamma v_\theta(S_{t+1}-v_\theta(S_t))</script></li>
  <li>This update ignores dependence of $v_\theta(S_{t+1})$ on $\theta$</li>
</ul>

<h3 id="action-value-function-approximation">Action-Value Function Approximation</h3>
<ul>
  <li>Approximate the action-value function<br />
<script type="math/tex">q_\theta(s,a)\approx q_\pi(S,a)</script></li>
  <li>For instance, with linear function approximation<br />
<script type="math/tex">q_\theta(s,a)=\phi(s,a)_\top\theta=\sum_{j=1}^n\phi_j(s,a)\theta_j</script></li>
  <li>Stochastic gradient descent update<br />
<script type="math/tex">% <![CDATA[
\begin{aligned}
\Delta\theta &= \alpha(q_\pi(s,a)-q_\theta(s,a))\nabla_\theta q_\theta(s,a) \\
&= \alpha(q_\pi(s,a)-q_\theta(s,a))\phi(s,a)
\end{aligned} %]]></script></li>
</ul>

<h3 id="least-squarse-prediction">Least Squarse Prediction</h3>
<hr />
<ul>
  <li>Given value function approximation $v_\theta(s) \approx v_\pi(s)$</li>
  <li>And <strong>experience</strong> $\mathcal{D}$ consisting of $\large \langle \text{state, estimated value} \rangle$pairs<br />
<script type="math/tex">\mathcal{D}=\big\{\langle S_1,\hat{v}_1^\pi \rangle,\langle S_2,\hat{v}_2^\pi \rangle,\ldots,\langle S_T,\hat{v}_T^\pi \rangle \big\}</script></li>
  <li>E.g., $\large \hat{V}<em>1^\pi=R</em>{t+1}+\gamma v_\theta(S_{t+1})$</li>
  <li>Which parameters $\theta$ give the best fitting value function $v_\theta(s)$?</li>
</ul>

<h3 id="stochastic-gradient-descent-with-experience-replay">Stochastic Gradient Descent with Experience Replay</h3>
<hr />
<p>Give experience consisting of $\large \langle \text{state, value} \rangle$pairs<br />
<script type="math/tex">\mathcal{D}=\big\{\langle S_1,\hat{v}_1^\pi \rangle,\langle S_2,\hat{v}_2^\pi \rangle,\ldots,\langle S_T,\hat{v}_T^\pi \rangle \big\}</script><br />
Repeat:</p>
<ol>
  <li>Sample state, value from experience<br />
<script type="math/tex">\langle s, \hat{v}_\pi  \rangle \sim \mathcal{D}</script></li>
  <li>Apply stochastic gradient decent update<br />
<script type="math/tex">\Delta\theta = \alpha(\hat{v}^\pi - v_\theta(s))\nabla_\theta v_\theta(s)</script><br />
Converges to least squares solution<br />
<script type="math/tex">\theta_\pi=\mathop{\text{argmin}}\limits_\theta LS(\theta)=\mathop{\text{argmin}}\limits_\theta\mathbb{E}_\mathcal{D}\big[(\hat{v}_i^\pi-v_\theta(S_i))^2\big]</script></li>
</ol>

<h3 id="linear-least-squares-prediction">Linear Least Squares Prediction</h3>
<hr />
<ul>
  <li>Experience replay finds least squares solution</li>
  <li>But it may take many iterations</li>
  <li>Using <strong>linear</strong> value function approximation $v_\theta(s)=\phi(s)^\top\theta$ we can solve the least squares solution directly</li>
  <li>At minimum of $LS(\theta)$, the expected update must be zero<br />
<script type="math/tex">% <![CDATA[
\begin{aligned}
\mathbb{E}_\mathcal{D}[\Delta\theta] &= 0 \\
\alpha\sum_{t=1}^T\phi_t(\hat{v}_t^\pi-\phi_t^\top\theta) &= 0 \\
\sum_{t=1}^T\phi_t\hat{v}_t^\pi &= \sum_{t=1}^T\phi_t\phi_t^\top\theta \\
\theta_t &= \Big(\sum_{t=1}^T\phi_t\phi_t^\top\Big)^{-1}\sum_{t=1}^T\phi_t\hat{v}_t^\pi
\end{aligned} %]]></script></li>
  <li>For N feature, direct solution time is $O(N^3)$</li>
  <li>Incremental solution time is $O(N^2)$ using Shermann-Morrison</li>
  <li>We do not know true values $v_\pi$ (have estimates $\hat{v}_t$)</li>
  <li>In practice, our “training data” must use noisy or biased sample of $v_\pi$</li>
</ul>

<p>      &lt;font color=blue&gt;<strong>LSMC</strong>&lt;/font&gt; Least Squares Monte-Carlo uses return<br />
<script type="math/tex">v_\pi \approx G_t</script>
      &lt;font color=blue&gt;<strong>LSTD</strong>&lt;/font&gt; Least Squares Temporal-Difference uses TD target<br />
<script type="math/tex">v_\pi \approx R_{t+1} + \gamma v_\theta(S_{t+1})</script></p>
<ul>
  <li>In each case we can solve directly for the fixed poine</li>
</ul>

<h3 id="deep-reinforcement-learning">Deep reinforcement learning</h3>
<hr />
<ul>
  <li>Many ideas immediately transfer when using deep neural networks:
    <ul>
      <li>TD and MC</li>
      <li>Double learning (e.g., double Q-learning)</li>
      <li>Experience replay</li>
      <li>…</li>
    </ul>
  </li>
  <li>Some ideas do not easily transfer
    <ul>
      <li>UCB</li>
      <li>Least squares TD/MC</li>
    </ul>
  </li>
</ul>

<h3 id="neural-q-learning">Neural Q-learning</h3>
<hr />
<ul>
  <li>Online neural Q-learning may include:
    <ul>
      <li>A <strong>network</strong> $q_\theta:\space O_t \Longrightarrow (q[1],\ldots,q[m])(m\space \text{actions})$</li>
      <li>An $\epsilon-\text{greedy}$ <strong>exploration policy</strong>: $q_t\space \Longrightarrow \space \pi_t \Longrightarrow \space A_t$</li>
      <li>A Q-learning <strong>loss function</strong> on $\theta$<br />
<script type="math/tex">I(\theta)=\frac{1}{2}\Big(R_{t+1}+\gamma\Big[\max_a q_\theta (S_{t+1},a)\Big] - q_\theta (S_t, A_t)\Big)^2</script><br />
where $[\cdot ]$ denotes stopping the gradient, so that the gradient is<br />
<script type="math/tex">\nabla_\theta I(\theta)=\Big(R_{t+1}+\gamma\max_a q_\theta(S_{t+1},a)-q_\theta(S_t,A_t)\Big)\nabla_\theta q_\theta(S_t,A_t)</script></li>
      <li>An <strong>optimizer</strong> to minimize the loss (e.g., SGD, RMSProp, Adma)</li>
    </ul>
  </li>
</ul>

<h3 id="dqn">DQN</h3>
<hr />

<ul>
  <li>DQN (Mnih et al. 2013, 2015) includes;
    <ul>
      <li>A <strong>network</strong> $q_\theta:\space O_t \mapsto (q[1],\ldots,q[m])(m\space \text{actions})$</li>
      <li>An $\epsilon-\text{greedy}$ <strong>exploration policy</strong>: $q_t\space \mapsto \space \pi_t \Longrightarrow \space A_t$</li>
      <li>A <strong>replay buffer</strong> to store and sample past transitions</li>
      <li>A <strong>target network</strong> $q_{\theta^-}:\space Q_t \mapsto\space(q^-[1],\ldots,q^-[m])$</li>
      <li>A Q-learning <strong>loss function</strong> on $\theta$ (use replay and target network) <br />
<script type="math/tex">I(\theta)=\frac{1}{2}\Big(R_{t+1}+\gamma\Big[\max_a q_{\theta^-} (S_{t+1},a)\Big] - q_\theta (S_t, A_t)\Big)^2</script></li>
      <li>An <strong>optimizer</strong> to minimize the loss (e.g., SGD, RMSProp, Adma)</li>
    </ul>
  </li>
  <li>Replay and target networks make RL look more like supervised learning</li>
  <li>It is unclear whether they are vital, but they help</li>
  <li>“DL-aware RL”</li>
</ul>

<h3 id="n-step-return">n-Step Return</h3>
<hr />
<ul>
  <li>
    <p>Consider the following n-steps returns for $n=1,2,\infty: $<br />
<script type="math/tex">% <![CDATA[
\begin{aligned}
&n=1 \quad &(TD)\quad & G_T^{(1)}=R_{t+1} + \gamma v(S_{t+1}) \\
&n=2 \quad &\quad & G_T^{(2)}=R_{t+1} + \gamma R_{t+2} +\gamma^2 v(S_{t+2}) \\
&\quad \vdots \quad &\quad & \vdots \\
&n=\infty \quad &(MC)\quad & G_T^{(\infty)}=R_{t+1} + \gamma R_{t+2}+\ldots+\gamma^{T-t-1}R_T
\end{aligned} %]]></script></p>
  </li>
  <li>Define the n-step return<br />
<script type="math/tex">G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n v(S_{t+n})</script></li>
  <li>n-step temporal-difference learning<br />
<script type="math/tex">v(S_t) \leftarrow v(S_t) + \alpha\Big(G_t^{(n)} - v(S_t)\Big)</script></li>
</ul>


<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

<script>
$(document).ready(function() {

    var toc = $('#TOC');

    // Select each header
    sections = $('.td-content h1');
        $.each(sections, function(idx, v) {
            section = $(v);
            var div_id = $(section).attr('id');
            var div_text = section.text().split('¶')[0];
            var parent = $("#" + div_id)
            var content = '<li id="link_' + div_id + '" class="md-nav__item"><a class="md-nav__link" href="#' + div_id + '" title="' + div_text +'">' + div_text +'</a></li>';
            $(toc).append(content);

            // Add section code to subnavigation
            var children = $('<nav class="md-nav"><ul class="md-nav__list"></nav></ul>')
            var contenders = $("#" + div_id).nextUntil("h1");
            $.each(contenders, function(idx, contender){
               if($(contender).is('h2') || $(contender).is('h3')) {
                   var contender_id = $(contender).attr('id');
                   var contender_text = $(contender).text().split('¶')[0];
                   var content = '<li class="md-nav__item"><a class="md-nav__link" href="#' + contender_id + '" title="' + contender_text +'">' + contender_text +'</a></li>';
                   children.append(content);
                }
             })
             $("#link_" + div_id).append(children);
        });
    });
</script>

<script>
var headers = ["h1", "h2", "h3", "h4"]
var colors = ["red", "orange", "green", "blue"]

$.each(headers, function(i, header){
    var color = colors[i];
    $(header).each(function () {
        var href=$(this).attr("id");
        $(this).append('<a class="headerlink" style="color:' + color + '" href="#' + href + '" title="Permanent link">¶</a>')
    });
})
</script>

<script>
$('h1').first().append('<div></div>')</script>

	
              
              <!-- <style>
  .feedback--answer {
    display: inline-block;
  }
  .feedback--answer-no {
    margin-left: 1em;
  }
  .feedback--response {
    display: none;
    margin-top: 1em;
  }
  .feedback--response__visible {
    display: block;
  }
</style>
<h5 class="feedback--title">Feedback</h5>
<p class="feedback--question">Was this page helpful?</p>
<button class="feedback--answer feedback--answer-yes">Yes</button>
<button class="feedback--answer feedback--answer-no">No</button>
<p class="feedback--response feedback--response-yes">
  Glad to hear it! Please <a href="/issues/new">tell us how we can improve</a>.
</p>
<p class="feedback--response feedback--response-no">
  Sorry to hear that. Please <a href="/issues/new">tell us how we can improve</a>.
</p>
<script>
  const yesButton = document.querySelector('.feedback--answer-yes');
  const noButton = document.querySelector('.feedback--answer-no');
  const yesResponse = document.querySelector('.feedback--response-yes');
  const noResponse = document.querySelector('.feedback--response-no');
  const disableButtons = () => {
    yesButton.disabled = true;
    noButton.disabled = true;
  };
  const sendFeedback = (value) => {
    if (typeof ga !== 'function') return;
    const args = {
      command: 'send',
      hitType: 'event',
      category: 'Helpful',
      action: 'click',
      label: window.location.pathname,
      value: value
    };
    ga(args.command, args.hitType, args.category, args.action, args.label, args.value);
  };
  yesButton.addEventListener('click', () => {
    yesResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(1);
  });
  noButton.addEventListener('click', () => {
    noResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(0);
  });
</script><br/>

 -->
           </div>
          </main>
        </div>
      </div>
      <footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        

</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="GitHub" data-original-title="GitHub">
    <a class="text-white" target="_blank" href="">
      <i class="fab fa-github"></i>
    </a>
  </li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
  <small class="text-white">© 2019 Yukai Wu All Rights Reserved</small>
  
  <p class="mt-2"><a href="/about/">About Docsy</a></p>	
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<script src="/assets/js/main.js"></script>

  </body>
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>  
</html>
