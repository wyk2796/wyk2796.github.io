<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yukai Wu</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 10 Jan 2020 02:27:21 -0500</pubDate>
    <lastBuildDate>Fri, 10 Jan 2020 02:27:21 -0500</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Planning and models</title>
        <description>&lt;h3 id=&quot;1-model-based-and-model-free-rl&quot;&gt;1. Model-Based and Model-Free RL&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Model-Free RL
    &lt;ul&gt;
      &lt;li&gt;No model&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; value function (and/or policy) from experience&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model-Based RL
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; a model from experience OR be given a model&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; value function (and/or policy) from model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;2-what-is-a-model&quot;&gt;2. What is a Model?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A model $\mathcal{M}&lt;em&gt;\eta$ is a representation of MDP $\langle \mathcal{S, A}, \hat{p}&lt;/em&gt;\eta \rangle$&lt;/li&gt;
  &lt;li&gt;For now, we will assume the states and actions are the same as in the real problem&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model &lt;strong&gt;approximates&lt;/strong&gt; the state transitions and rewards $\hat{p}_\eta\approx p$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;R_{t+1}, S_{t+1} \sim \hat{p}_\eta (r, s'|S_t,A_t)&lt;/script&gt;&lt;br /&gt;
(Note there is not probability distribution function)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Optionally, we could model rewards and state dynamics separately&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;3-model-learning&quot;&gt;3. Model Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: &lt;strong&gt;estimate&lt;/strong&gt; model $\mathcal{M}_\eta$ from experienve ${S_1, A_1, R_2,\ldots,S_T}$&lt;/li&gt;
  &lt;li&gt;This is a supervised learning problem&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
S_1,A_1 &amp;\rightarrow R_2, S_2 \\
&amp;\vdots \\
S_{T-1},A_{T-1} &amp;\rightarrow R_{T}, S_{T}
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Learn a function $f(s,a) = r,s’$&lt;/li&gt;
  &lt;li&gt;Pick loss function (e.g. mean-squared error), and find parameters $\eta$ that minimise empirical loss&lt;/li&gt;
  &lt;li&gt;This would give an expectation model&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;If $f(s,a)=r,s’$, then we would hope $s’\approx \mathbb{E}[S_{t+1}&lt;/td&gt;
          &lt;td&gt;s=S_t,a=A_t]$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;4-stochastic-model&quot;&gt;4. Stochastic Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;We may not want to assume everything is linear&lt;/li&gt;
  &lt;li&gt;Then expected states may not be roght — they may not correspond to actual states, and iterating the model may do weird things.&lt;/li&gt;
  &lt;li&gt;Alternative: &lt;strong&gt;stochastic models&lt;/strong&gt; (also know as &lt;strong&gt;generative models&lt;/strong&gt;) 
&lt;script type=&quot;math/tex&quot;&gt;\hat{R}_{t+1}, \hat{S}_{t+1} = \hat{p}(S_t, A_t,\omega)&lt;/script&gt;
where $\omega$ is a noise term&lt;/li&gt;
  &lt;li&gt;Stochastic models can be chained, even if the model is non-linear.&lt;/li&gt;
  &lt;li&gt;But they do add not noise&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;5-exmples-of-models&quot;&gt;5. Exmples of Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Table Looking Model&lt;/li&gt;
  &lt;li&gt;Linear Expectation Model&lt;/li&gt;
  &lt;li&gt;Linear Gaussian Model&lt;/li&gt;
  &lt;li&gt;Deep Neural Network Model&lt;/li&gt;
  &lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;51-table-lookup-model&quot;&gt;5.1 Table Lookup Model&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Model is an explicit MDP&lt;/li&gt;
  &lt;li&gt;Count visits $N(s,a)$ to each state action pair&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\hat{p}_t(s'|s,a) &amp;= \frac{1}{N(s,a)}\sum_{k=0}^{t-1}I(S_k=s,A_k=a,S_{k+1}=s') \\
\mathbb{\hat{p}_t}[R_{t+1}|S_t=s,A_t=a] &amp;= \frac{1}{N(s,a)}\sum_{k=0}^{t-1}I(S_k=s,A_k=a)R_{k+1}
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Alternatively, use non-parameteric ‘replay’
    &lt;ul&gt;
      &lt;li&gt;At each time-step t, record experience tuple $\langle S_t,A_t,R_{t+1},S_{t+1} \rangle$&lt;/li&gt;
      &lt;li&gt;To sample model, randomly pick tuple matching $\langle s,a,\cdot,\cdot\rangle$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;6-planing-with-a-model&quot;&gt;6. Planing with a Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Given a model $\hat{p}_\eta$&lt;/li&gt;
  &lt;li&gt;Solve the MDP $\langle \mathcal{S,A},\hat{p}_\eta \rangle$&lt;/li&gt;
  &lt;li&gt;Using favourite planning algorithm
    &lt;ul&gt;
      &lt;li&gt;Value iteration&lt;/li&gt;
      &lt;li&gt;Policy iteration&lt;/li&gt;
      &lt;li&gt;Tree search&lt;/li&gt;
      &lt;li&gt;……&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;61-sample-based-planning&quot;&gt;6.1 Sample-Based Planning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;A simple but powerful approach to planning&lt;/li&gt;
  &lt;li&gt;Use the model &lt;strong&gt;only&lt;/strong&gt; to generate sampler&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sample&lt;/strong&gt; experience from model&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;S,R \sim \hat{p}_\eta(\cdot|s,a)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Apply &lt;strong&gt;model-free&lt;/strong&gt; RL to sample, e.g.:
    &lt;ul&gt;
      &lt;li&gt;Monte-Carlo control&lt;/li&gt;
      &lt;li&gt;Sarsa&lt;/li&gt;
      &lt;li&gt;Q-learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;7-conventional-model-based-and-model-free-metheds&quot;&gt;7. Conventional model-based and model-free metheds&lt;/h3&gt;

&lt;p&gt;Traditional RL algorithms did not explicitly store their experiences, and were often placed into one of two groups.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Model-free&lt;/strong&gt; methods update the value function and/or policy and do not have explicit dynamics models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Models-based&lt;/strong&gt; methods update the transition and reward models, and compute a value function or policy from the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;8-using-experience-in-the-place-of-model&quot;&gt;8. Using experience in the place of model&lt;/h3&gt;
&lt;p&gt;Recall prioritized sweeping from tabular dynamic programming&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Update the value function of the states with the largest magnitude Bellman errors using a priority queue.
A related idea is prioritized experience replay (Schaul et al, 2015) which works from experience for general function approximation.&lt;/li&gt;
  &lt;li&gt;The experience replay buffer maintain a priority for each transition, with the priority given by the magnitude of the Bellman error.&lt;/li&gt;
  &lt;li&gt;Minibatches are sampled using this priority to quickly reduce errors.&lt;/li&gt;
  &lt;li&gt;Weighted importance sampling corrects for bias from non-uniform sampling&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;9-limits-of-planning-with-an-inaccurate-model&quot;&gt;9. Limits of Planning with an Inaccurate Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Given an imperfect model $\hat{p}_\eta \neq p$&lt;/li&gt;
  &lt;li&gt;Performance is limited to optimal policy for approximate MDP $\langle \mathcal{M,A,\hat{p}_\eta}\rangle$&lt;/li&gt;
  &lt;li&gt;Model-based RL is only as good as the estimated model&lt;/li&gt;
  &lt;li&gt;When the model is inaccurate, planning process will compute a suboptimal policy (not covered in these slides)
    &lt;ul&gt;
      &lt;li&gt;Approach 1: when model is wrong, use model-free RL&lt;/li&gt;
      &lt;li&gt;Approach 2: reson explicitly about model uncertainty over $\eta$ (e.g. Bayesian methon)&lt;/li&gt;
      &lt;li&gt;Approach 3: Combine model-based and model-free methods in a safe way.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;10-real-and-simulated-experience&quot;&gt;10. Real and Simulated Experience&lt;/h3&gt;
&lt;p&gt;We consider two sources of experience
Real experience Sampled from environment (true MDP)&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;r,s'\sim p&lt;/script&gt;&lt;br /&gt;
Sumulated experience Sampled from model (approximate MDP)&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;r,s' \sim \hat{p}_\eta&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;11-intergrating-learning-and-planning&quot;&gt;11. Intergrating Learning and Planning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Model-Free RL
    &lt;ul&gt;
      &lt;li&gt;No model&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; value function (and/or policy) from real experience&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model-Based RL (using Sample-Based Planning)
    &lt;ul&gt;
      &lt;li&gt;Learn a model from real experience&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; value function (and/or policy) from simulated experience&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dyna
    &lt;ul&gt;
      &lt;li&gt;Learn a model from real experience&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Learn AND plan&lt;/strong&gt; value function (and/or policy) from real and simulated experience&lt;/li&gt;
      &lt;li&gt;Treat real and sumulated esperience equivalently. Conceptually, the update from learning or planning are not distinguished.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;111-dyna-q-algorithm&quot;&gt;11.1 Dyna-Q Algorithm&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \text{Initialize}\space Q(s,a)\space \text{and Model}(s,a)\quad \text{for all}\space s\in\mathcal{A}(s) \\
&amp; \text{Do forever:} \\
&amp; \qquad (a) \quad s \leftarrow \text{current (nonterminal) state} \\
&amp; \qquad (b)\quad a \leftarrow \epsilon\text{-greedy}(s,Q) \\
&amp; \qquad (c) \quad \text{Execute action}\space a\text{; observe resultant state, }s'\text{, and reward, }r\\
&amp; \qquad (d)\quad Q(s,a)\leftarrow Q(s,a)+\alpha[r + \gamma\max_{a'}Q(s',a')-Q(s,a)]\\
&amp; \qquad (e)\quad Model(s,a) \leftarrow s',r \quad\text{(assuming deterministic environment)}\\
&amp; \qquad (f)\quad \text{Repeat}\space N \space \text{times:} \\
&amp; \qquad\qquad s \leftarrow \text{random previously observed state} \\
&amp; \qquad\qquad a\leftarrow \text{random action previously taken in}\space s \\
&amp; \qquad\qquad s',r \leftarrow Model(s,a) \\
&amp; \qquad\qquad Q(s,a)\leftarrow Q(s,a) + \alpha[r+\gamma\max_{a'}Q(s',a')-Q(s,a)]
\end{aligned} %]]&gt;&lt;/script&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;112-dyna-with-function-approximation&quot;&gt;11.2 Dyna with Function Approximation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How can an agent plan when the actual environmental states are not know?&lt;/li&gt;
  &lt;li&gt;Can directly approximate probability distributions of the transitions and the rewards.&lt;/li&gt;
  &lt;li&gt;Probability distribution models in high dimensional feature spaces are computationally expensive and often inaccurate!&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;12-planning-for-action-selection&quot;&gt;12. Planning for Action Selection&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;We considered the case where planning is used to improve a global value function&lt;/li&gt;
  &lt;li&gt;Now consider planning for the near future, to select the next action&lt;/li&gt;
  &lt;li&gt;The distribution of states that may be encounted from &lt;strong&gt;now&lt;/strong&gt; can diff from the distribution of states encountered from a starting state&lt;/li&gt;
  &lt;li&gt;The agent may be able to make a more accurate local value function (for the states that will be encountered soon) than the global value function&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;121-forward-search&quot;&gt;12.1 Forward Search&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Forward search&lt;/strong&gt; algorithms select the best action by &lt;strong&gt;look ahead&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;They build a search tree with the current state $s_t$ at the root&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using a &lt;strong&gt;model&lt;/strong&gt; of the MDP to look ahead
&lt;img src=&quot;https://i.loli.net/2019/12/24/QkTADdw2EPx4zyG.png&quot; alt=&quot;tree&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;No need to solve whole MDP, just sub-MDP starting from &lt;strong&gt;now&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;122-simulation-based-search&quot;&gt;12.2 Simulation-Based Search&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Forward&lt;/strong&gt; search paradigm using sample-based planning&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Simulate&lt;/strong&gt; episodes of experience from &lt;strong&gt;now&lt;/strong&gt; with the model&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Apply &lt;strong&gt;model-free&lt;/strong&gt; RL to simulated episodes&lt;br /&gt;
&lt;img src=&quot;https://i.loli.net/2019/12/24/JmliqxjnY7TB5fI.png&quot; alt=&quot;Search Path&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Simulate episode of experience from &lt;strong&gt;now&lt;/strong&gt; with the model&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\{S_t^k, A_t^k,R_{t+1}^k,\ldots,S_T^k\}_{k=1}^K \sim \hat{p}_\eta&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Apply &lt;strong&gt;model-free&lt;/strong&gt; RL to sumulated episodes
    &lt;ul&gt;
      &lt;li&gt;Monte-carlo control $\rightarrow$ Monte-Carlo search&lt;/li&gt;
      &lt;li&gt;Sarsa $\rightarrow$ TD search&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;123-search-tree-vs-value-function-approximation&quot;&gt;12.3 Search tree vs. value function approximation&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Search tree is a table lookup approach&lt;/li&gt;
  &lt;li&gt;Based on a partial instantiation of the table&lt;/li&gt;
  &lt;li&gt;For model-free reinforcement learning, table lookup is naive
    &lt;ul&gt;
      &lt;li&gt;Can’t store value for all states&lt;/li&gt;
      &lt;li&gt;Doesn’t generalise between similar states&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;For simulation-based search, table lookup is less naive
    &lt;ul&gt;
      &lt;li&gt;Search tree stores value for easily reachable states&lt;/li&gt;
      &lt;li&gt;In huge search spaces, value function approximation is helpful&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;13-monte-carlo-simulation&quot;&gt;13. Monte-Carlo Simulation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Given a parameterized model $\mathcal{M}_\eta$ and a &lt;strong&gt;simulation policy&lt;/strong&gt; $\pi$&lt;/li&gt;
  &lt;li&gt;Simulate $K$ episodes from current state $S_t$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\{S_t^k=S_t,A_t^k,R_{t+1}^k,S_{t+1}^k,\ldots,S_t^k\}_{k=1}^K \sim \hat{p}_\eta, \pi&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Evaluate state by mean return (&lt;strong&gt;Monte-Carlo evaluaiton&lt;/strong&gt;)&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v(\color{red}{S_t})=\frac{1}{K}\sum_{k=1}^K G_t^k \rightsquigarrow v_\pi(S_t)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;131-simple-monte-carlo-search&quot;&gt;13.1 Simple Monte-Carlo Search&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Given a model $\mathcal{M}_\eta$ and a policy $\pi$&lt;/li&gt;
  &lt;li&gt;For each action $a \in \mathcal{A}$
    &lt;ul&gt;
      &lt;li&gt;Simulate $K$ episodes from current (real) state $s$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\{S_t^k=s,A_t^k=a,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\ldots,S_t^k\}_{k=1}^K \sim \mathcal{M}_v,\pi&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Evaluate actions by mean return (Monto-Carlo evaluation)&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;q(\color{red}{s,a})=\frac{1}{K}\sum_{k=1}^K G_t^k \rightsquigarrow q_\pi(S_t)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Select current (real) action with maximum value&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;A_t=\mathop{\text{argmax}}\limits_{a\in\mathcal{A}}q(S_t,a)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;132-monte-carlo-tree-search-evaluation&quot;&gt;13.2 Monte-Carlo Tree Search (Evaluation)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Given a model $\mathcal{M}_\eta$&lt;/li&gt;
  &lt;li&gt;Simulate $K$ episodes from current state $S_t$ using current simulation policy $\pi$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\{\color{red}{S_t^k=S_t},A_t^k,R_{t+1}^k,S_{T+1}^k,\ldots,S_t^k\}_{k=1}^K \sim \mathcal{M}_v,\pi&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Build a search tree containing visited states and actions&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluate&lt;/strong&gt; states $q(s,a)$ by mean return of eqisodes from $s,a$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;q(s,a)=\frac{1}{N(s,a)}\sum_{k=1}^K\sum_{u=t}^T1(S_u^k,A_u^k=s,a)(G_u^k \rightsquigarrow q_\pi(s,a))&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;After searching, select current (real) action with maximum value in search tree&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;a_t=\mathop{\text{argmax}}\limits_{a\in\mathcal{A}}q(S_t,a)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;monte-carlo-tree-search-simulation&quot;&gt;Monte-Carlo Tree Search (Simulation)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;In MCTS, the simulation policy $\pi$ &lt;strong&gt;improves&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The simulation policy $\pi$ has two phases (in-tree, out-of-tree)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Tree policy&lt;/strong&gt; (improves): pick action from $q(s,a)$ (e.g. $\epsilon$-greedy($q(s,a)$))&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Rollout policy&lt;/strong&gt; (fixed): e.g., pick actions randomly&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Repeat (for each simulated episode)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt; actions in tree according to tree policy&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Expand&lt;/strong&gt; search tree by one node&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Rollout&lt;/strong&gt; to termination with default policy&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Update&lt;/strong&gt; action-values $q(s,a)$ in the tree&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Output best action when simulation time runs out.&lt;/li&gt;
  &lt;li&gt;With some asumptions, converges to the optimal values, $q(s,a)\Rightarrow q_*(s,a)$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 23 May 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2019/Planning-and-Models/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/Planning-and-Models/</guid>
        
        
      </item>
    
      <item>
        <title>Policy gradients and actor critic</title>
        <description>&lt;h3 id=&quot;general-overview&quot;&gt;General Overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Model-based RL:&lt;br /&gt;
 + ‘Easy’ to learn a model (supervised learning) &lt;br /&gt;
 + Learns ‘all there is to know’ from the data&lt;br /&gt;
 - Objective capture irrelevant information&lt;br /&gt;
 - May focus compute/capacity on irrelevant detail&lt;br /&gt;
 - Computing policy (planning) is non-trivial and can be computationally expensive&lt;/li&gt;
  &lt;li&gt;Valued-based RL:&lt;br /&gt;
 + Closer to true objective
 + Fairly well-understood — somewhat similar to regression&lt;br /&gt;
 - Still not the true objective — may still focus capacity on less-important details&lt;/li&gt;
  &lt;li&gt;Policy-based RL:&lt;br /&gt;
 + Right objective!
 - Ignores other learnable knowledge (potentially not the most efficient use of data)&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;advantage-of-policy-based-rl&quot;&gt;Advantage of Policy-Based RL&lt;/h3&gt;
&lt;p&gt;Advantages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Good convergence properties&lt;/li&gt;
  &lt;li&gt;Easily extended to high-dimensional or continuous action spaces&lt;/li&gt;
  &lt;li&gt;Can learn &lt;strong&gt;stochastic&lt;/strong&gt; policies&lt;/li&gt;
  &lt;li&gt;Sometimes Policies are &lt;strong&gt;simple&lt;/strong&gt; while values and models are complex
    &lt;ul&gt;
      &lt;li&gt;E.g., rich domain, but optimal is always go left.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Disadvantages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Susceptible to local optimal (expecially with non-linear FA)&lt;/li&gt;
  &lt;li&gt;Obtained knowledge is specific, does not always generalize well&lt;/li&gt;
  &lt;li&gt;Ignores a lot of information in the data (when used in isolation)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;policy-objective-functions&quot;&gt;Policy Objective Functions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: given policy $\pi_{\theta}(s,a)$ with patameters $\theta$, find best $\theta$&lt;/li&gt;
  &lt;li&gt;But how do we measure the quality of a policy $\pi_{\theta}$?&lt;/li&gt;
  &lt;li&gt;In episodic environments we can use the &lt;strong&gt;start value&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;J_{1}(\theta)=v_{\pi_{\theta}}(s_{1})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;In continuing environments we can use the average value&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;J_{avV}(\theta)=\sum_{s}\mu_{\pi_{\theta}}(s)v_{\pi_{\theta}}(s)&lt;/script&gt;&lt;br /&gt;
where $\mu_{\pi}(s)=p(S_{t}=s|\pi)$ is the probability of being in state s in the long run
Think of is as the ratio of time spent in $s$ under policy $\pi$&lt;/li&gt;
  &lt;li&gt;Or the average reward per time-step&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;J_{avR}(\theta)=\sum_{s}\mu_{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(s,a)\sum_{r}p(r|s,a)r&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;gradients-on-parameterized-policies&quot;&gt;Gradients on Parameterized Policies&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;We need to compute an estimate of the policy gradient&lt;/li&gt;
  &lt;li&gt;Assume policy $\pi_{\theta}$ is differentiable almost everywhere
    &lt;ul&gt;
      &lt;li&gt;E.g., $\pi_{\theta}$is a linear function of the agent state, or a neural network&lt;/li&gt;
      &lt;li&gt;Or we could have a parameterized class of controllers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Goal is to compute&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}J(\theta)=\nabla\mathbb{E}_{d}[v_{\pi_{\theta}}(s)]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;We will use Monte Carlo samples to compute this gradient&lt;/li&gt;
  &lt;li&gt;So, how does $\mathbb{E}&lt;em&gt;{d}[v&lt;/em&gt;{\pi_{\theta}}(S)]$ depend on $\theta$?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The policy gradient approach also applies to (multi-step) MDPs&lt;/li&gt;
  &lt;li&gt;Replaces instantaneous reward R with long-term value $q_{\pi}(s,a)$&lt;/li&gt;
  &lt;li&gt;Policy gradient theorem applies to start state objective, average reward and average value objective&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;
For any differentiable policy $\pi_{\theta}(s,a)$, for any of the policy objective functions $J=J_{1}, J_{avR}, or 1\frac{1}{1-\gamma}J_{avV}$, the policy gradient is&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}J(\theta) = \mathbb{E}[q_{\pi_{\theta}}(S,A)\nabla_{\theta}\log\pi_{\theta}(A|S)]&lt;/script&gt;&lt;br /&gt;
Expectation is over both states and actions&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;policy-gradients-on-trajectories-derivation&quot;&gt;Policy Gradients on trajectories: Derivation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Consider trajectory $\zeta=S_0,A_0,R_0,S_1,A_1,R_1,S_2,\ldots$ with return $G(\zeta)$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \nabla_\theta J_\theta(\pi)=\nabla_\theta\mathbb{E}[G(\zeta)]=\mathbb{E}[G(\zeta)\nabla_\theta\log p(\zeta)]\qquad \text{(score function trick)} \\
&amp; \nabla_\theta\log p(\zeta) \\
&amp; =\nabla_\theta\log\Big[p(S_0)\pi(A_0|S_0)p(S_1|S_0,A_0)\pi(A_1|S_1)\cdots)\Big] \\
&amp; =\nabla_\theta\Big[\log p(S_0)+\log\pi(A_0|S_0)+\log p(S_1|S_0,A_0)+\log\pi(A_1|S_1)+\cdots)\Big] \\
&amp; =\nabla_\theta\Big[\log\pi(A_0|S_0)+\log\pi(A_1|S_1)+\cdots\Big] \\
\textbf{So:} \\
&amp; \nabla_\theta J_\theta(\pi)=\mathbb{E}\Big[G(\zeta)\nabla_\theta\sum_{t=0}\log\pi(A _t|S_t)\Big] = \mathbb{E}\Big[\Big(\sum_{t=0}R_{t+1}\Big)\Big(\nabla_\theta\sum_{t=0}\log\pi(A_t|S_t)\Big)\Big]
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;policy-gradients-on-trajectories-reduce-variance&quot;&gt;Policy gradients on trajectories: reduce variance&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Note that, in general &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathbb{E}[b\nabla_{\theta}\log\pi(A_{t}|S_{t})]&amp;=\mathbb{E}\Bigg[\sum_{a}\pi(a|S_{t})b\nabla_{\theta}\log\pi(a|S_{t})\Bigg] \\
&amp;=\mathbb{E}\Bigg[b\nabla_{\theta}\sum_{a}\pi(a|S_{t})\Bigg] \\
&amp;=\mathbb{b}[b\nabla_{\theta}1] \\
&amp;=0
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;The sum of probability distribution is 1&lt;/li&gt;
  &lt;li&gt;This holds only if $b$ does not depend on the action (though it can depend on the state)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implies we can subtract a &lt;strong&gt;baseline&lt;/strong&gt; to reduce variance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider trajactory $\zeta=S_{0},A_{0},R_{0},S_{1},A_{1},R_{1},S_{2},\ldots$ with return $G{\zeta}$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}J_{\theta}(\pi)=\mathbb{E}\Bigg[\Bigg(\sum_{t=0}R_{t+1}\Bigg)\Bigg(\nabla_{\theta}\sum_{t=0}log\pi(A_{t}|S_{t})\Bigg)\Bigg]&lt;/script&gt;  &lt;br /&gt;
but $\sum_{t=0}^{k}R_{t+1}$ does not depend on actions $A_{k+1}, A_{k+2},\cdots,$so&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;=\mathbb{E}\Bigg[\sum_{t=0}\nabla_{\theta}\log\pi(A_{t}|S_{t})\sum_{i=0}R_{i+1}\Bigg] \\
&amp;=\mathbb{E}\Bigg[\sum_{t=0}\nabla_{\theta}\log\pi(A_{t}|S_{t})\sum_{i=\color{red}{t}}R_{i+1}\Bigg] \\
&amp;=\mathbb{E}\Bigg[\sum_{t=0}\nabla_{\theta}\log\pi(A_{t}|S_{t})q_{\pi}(S_{t},A_{t})\Bigg]
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A good baseline is $v_{\pi}(S_{t})$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\nabla_{\theta}J_{\theta}(\pi)=\mathbb{E}\Bigg[\sum_{t=0}\nabla_{\theta}\log\pi(A_{t}|S_{t})q_{\pi}(S_{t},A_{t})-v_{\pi}(S_{t})\Bigg]&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Typically, we estimate $v_{w}(s)$ explicitly, and sample&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(S_{t},A_{t})\approx G_{t}^{(n)}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For instance, $G_{t}^{(1)}=R_{t+1}+ \gamma v_{w}(S_{s+1})$&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;estimating-the-action-value-function&quot;&gt;Estimating the Action-Value Function&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The Critic is solving a familiar problem: policy evaluation&lt;/li&gt;
  &lt;li&gt;What is the value of policy $\pi_{\theta}$ for current parameters $\pi$&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;actor-critic&quot;&gt;Actor-Critic&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Critic&lt;/strong&gt; Update parameters $w$ of $v_{w}$ by n-step TD (e.g., $n=1$)
&lt;strong&gt;Actor&lt;/strong&gt; Update $\theta$ by policy gradient&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \textbf{function}\space\text{ADVANTAGE ACTOR CRITIC} \\
&amp; \qquad\text{Initialise}\quad s, \theta \\
&amp; \qquad\textbf{for}\quad t=0,1,2,\ldots\space \textbf{do} \\
&amp; \qquad\qquad \text{Sample}\space A_t \sim \pi_\theta(S_t) \\
&amp; \qquad\qquad \text{Sample}\space R_{t+1}\space\text{and}\space S_{t+1} \\
&amp; \qquad\qquad \delta_t=R_{t+1}+\gamma v_w(S_{t+1})-v_w(S_t) \qquad \text{[one-step TD-error, or \color{red}{advantage}]}\\
&amp; \qquad\qquad w\leftarrow w+\beta\delta_t\nabla_wv_w(S_t) \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad \text{[TD(0)]} \\
&amp; \qquad\qquad \theta\leftarrow\theta+\alpha\delta_t\nabla_\theta\log\pi_\theta(A_t|S_t) \qquad\qquad\qquad\quad\space\space\thinspace\text{[Policy gradient update]}\\
&amp; \qquad\textbf{end for} \\
&amp; \textbf{end function}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;full-advantage-actor-critic-agent&quot;&gt;Full Advantage Actor Critic Agent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Adventage actor critic include
    &lt;ul&gt;
      &lt;li&gt;A &lt;strong&gt;representation&lt;/strong&gt; (e.g., LSTM): $(S_{t-1}, O_t)\rightarrow S_t$&lt;/li&gt;
      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $v_w: \space S\rightarrow v$&lt;/li&gt;
      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $\pi_\theta:\space S\rightarrow \pi$&lt;/li&gt;
      &lt;li&gt;Copies/variants $\pi^m$ of $\pi_\theta$ to use as &lt;strong&gt;policies&lt;/strong&gt;: $S_t^m\rightarrow A_t^m$&lt;/li&gt;
      &lt;li&gt;A n-step TD &lt;strong&gt;loss&lt;/strong&gt; on $v_w$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;I(w)=\frac{1}{2}\Big(G_t^{(n)}-v_w(S_t)\Big)^2&lt;/script&gt;&lt;br /&gt;
where $G_t^{(n)} = R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}v_w(S_{t+n})$&lt;/li&gt;
      &lt;li&gt;A n-step REINFORCE &lt;strong&gt;loss&lt;/strong&gt; on $\pi_\theta$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;I(\theta)=\Big[G_t^{(n)}-v_w(S_t)\Big]\log\pi_\theta(A_t|S_t)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Optimizers&lt;/strong&gt; to minimize the losses&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Also know as A2C, or A3C (when combined with asynchronous parameter update)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;bias-in-actor-critic-algorithm&quot;&gt;Bias in Actor-Critic Algorithm&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Approximating the policy gradient introduce bias&lt;/li&gt;
  &lt;li&gt;A biased policy gradient may not find the right solution&lt;/li&gt;
  &lt;li&gt;Full return: high variance&lt;/li&gt;
  &lt;li&gt;One-step TD-error: high bias&lt;/li&gt;
  &lt;li&gt;n-step TD-error: useful middle ground&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\delta_t^{(n)} &amp;= G_t^{(n)} - v_w(S_t) \\
&amp;=\underbrace{R_{t+1} + \gamma R_{t+2}+ \cdots + \gamma^{n-1}R_{t+n} + \gamma^{n}v_w(S_t+n)}_{=G_t^{(n)}}-v_w(S_t)
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;It is really important to use close-to on-policy targets&lt;/li&gt;
  &lt;li&gt;If needed, use importance sampling to correct&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;G_t^{(n),\rho} = \frac{\pi_\theta(A_t|S_t)}{b(A_t|S_t)}\Big(R_{t+1}=\gamma_{t+1}^{(n-1),\rho}\Big)&lt;/script&gt;&lt;br /&gt;
with&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;G_t^{(0),\rho} =v_w(S_t) \approx v_\pi(S_t)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;large-colordarklambdatextbf-returns&quot;&gt;$\Large \color{dark}{\lambda}\textbf{-returns}$&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;We can write a multi-step return recursively&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; G_t^{(n)}=R_{t+1}+\gamma G_{t+1}^{n-1} \\
&amp; G_t^{(0)} = v_w(S_t) \approx v_\pi(S_t)
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;This is equivalent to &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;G_t^\lambda = R_{t+1} + \gamma(1- \lambda_{t+1})v_w(S_{t+1}) + \gamma\lambda_{t+1}G_{t+1}^\lambda&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;We can generalize to $\lambda_t \in [0,1]$; this is called a $\lambda-\text{return}$&lt;/li&gt;
  &lt;li&gt;It can be interpreted as a &lt;strong&gt;mixture of n-step returns&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;One way to correct for off-policy returns: bootstrap (set $\lambda=0$) whenever the policies differ&lt;/li&gt;
  &lt;li&gt;Can be used for policy-gradient and value prediction&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;trust-region-policy-optimization&quot;&gt;Trust Region Policy Optimization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Many extensions and variants exist&lt;/li&gt;
  &lt;li&gt;Important: be careful with update: a bad policy leads to bad data&lt;/li&gt;
  &lt;li&gt;This is different from supervised learning (where learning and data are independent)&lt;/li&gt;
  &lt;li&gt;One solution: regularise policy to not change too much&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;increasing-robustness-with-trust-regions&quot;&gt;Increasing Robustness with Trust Regions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;One way to prevent instability is to &lt;strong&gt;regularise&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;A popular method is to &lt;strong&gt;limit the difference between subsequent policies&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;For instance, use the Kullbeck-Leibler divergence:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;KL(\pi_{old}\|\pi_\theta)=\mathbb{E}\Big[\int\pi_{old}(a|S)\log\frac{\pi_\theta(a|S)}{\pi_{old}(a|S)}da\Big]&lt;/script&gt;&lt;br /&gt;
(a divergence is like a distance — but between distributions)&lt;/li&gt;
  &lt;li&gt;Then maximise $J(\theta)-\eta KL(\pi_{old}|\pi_\theta)$, for some small $\eta$&lt;/li&gt;
  &lt;li&gt;It can also help to use large batches \
 c.f &lt;strong&gt;TRPO&lt;/strong&gt; (Schulman et al. 2015) and &lt;strong&gt;PPO&lt;/strong&gt; (Abbeel &amp;amp; Schulman 2016)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;gaussian-policy&quot;&gt;Gaussian POlicy&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In conitnuous action spaces, a Gaussian policy is common&lt;/li&gt;
  &lt;li&gt;E.g., mean is some function of state $\mu(s)$&lt;/li&gt;
  &lt;li&gt;For simplicity, lets consider fixed variance of $\sigma^2$ (can be parameterized as well, instead)&lt;/li&gt;
  &lt;li&gt;Policy is Gaussian, $a \sim \mathcal{N}(\mu(s),\sigma^2)$&lt;/li&gt;
  &lt;li&gt;The gradient of the log of the policy is then&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta\log\pi_\theta(s,a)=\frac{a-\mu(s)}{\sigma_2}\nabla\mu(s)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;This can be used, for instance, in REINFORCE / advandage actor critic&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;continuous-actor-critic-learning-automaton-cacla&quot;&gt;Continuous Actor-Critic Learning Automaton （Cacla）&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \blacktriangleright a_t=Actor_\theta(S_t)\qquad  &amp;\text{(get current (continuous) action proposal)}&amp;\\
&amp; \blacktriangleright A_t\sim\pi(\cdot|S_t,a_t)\space(e.g.,A_t \sim \mathcal{N}P(a_t, \sum) \qquad &amp;\text{(explore)}&amp;\\
&amp; \blacktriangleright \delta_t=R_{t+1}+\gamma v_w(S_{t})\qquad &amp;\text{(compute TD error)} &amp;\\
&amp; \blacktriangleright \text{Update}\space v_w{S_t}\space(\text{e.g.,using TD}) \qquad&amp;\text{(policy evaluation)} &amp;\\
&amp; \blacktriangleright \text{If}\space \delta_t &gt; 0, \text{update Actor}_\theta(S_t)\space\text{towards}\space A_t \qquad&amp;\text{(policy improvement)}&amp;\\
&amp; \blacktriangleright \text{if}\space \delta_t \leqslant 0, \text{do not update Actor}_\theta \qquad&amp;\text{} &amp; 
\end{aligned} %]]&gt;&lt;/script&gt;
</description>
        <pubDate>Fri, 17 May 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2019/Policy-Gradients-and-Actor-Critic/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/Policy-Gradients-and-Actor-Critic/</guid>
        
        
      </item>
    
      <item>
        <title>Function approximation</title>
        <description>&lt;h3 id=&quot;introduce&quot;&gt;Introduce&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;policy&lt;/strong&gt;, &lt;strong&gt;value function&lt;/strong&gt; and &lt;strong&gt;model&lt;/strong&gt; are all functions&lt;/li&gt;
  &lt;li&gt;We want to learn (one of) these from experience&lt;/li&gt;
  &lt;li&gt;If there are too many states, we need to approximate&lt;/li&gt;
  &lt;li&gt;In general, this is called RL with function approximation&lt;/li&gt;
  &lt;li&gt;When using deep neural nets, this is often called deep reinforcement learning&lt;/li&gt;
  &lt;li&gt;The term is fairly new — the combination is decades old&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;
&lt;h3 id=&quot;value-function-approximation&quot;&gt;Value Function Approximation&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;lookup tables&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Every state $s$ has an entry $q(s,a)$&lt;/li&gt;
      &lt;li&gt;Or every state-action pair $s,a$ has an entry $q(s,a)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Large MDPs:
    &lt;ul&gt;
      &lt;li&gt;There are too many states and/or actions to store in memory&lt;/li&gt;
      &lt;li&gt;It is too slow to learn the value of each state individually&lt;/li&gt;
      &lt;li&gt;Individual states are often &lt;strong&gt;not fully observable&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Solution for large MDPs:
    &lt;ul&gt;
      &lt;li&gt;Estimate value function with &lt;strong&gt;function approximation&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
v_\theta(s)\approx v_\pi(s) \qquad &amp;(or, v_*(s)) \\
q_\theta(s,a)\approx q_\pi(s,a) \qquad &amp;(or,q_*(s,a))
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Generalise&lt;/strong&gt; from seen states to unseen states&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Update&lt;/strong&gt; parameter $\theta$ using MC or TD observable&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If the environement state is not fully observable:
    &lt;ul&gt;
      &lt;li&gt;Use the &lt;strong&gt;agent state&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Consider learning a &lt;strong&gt;state update function&lt;/strong&gt; $S_{t+1}=u(S_t,O_{t+1})$&lt;/li&gt;
      &lt;li&gt;Henceforth, $S_t$ denotes the agent state&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;which-function-approximator&quot;&gt;Which Function Approximator?&lt;/h3&gt;
&lt;p&gt;There are many function approximators, e.g.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Artificial neural network&lt;/li&gt;
  &lt;li&gt;Decision tree&lt;/li&gt;
  &lt;li&gt;Nearest neighbour&lt;/li&gt;
  &lt;li&gt;Fourier / wavelet bases&lt;/li&gt;
  &lt;li&gt;Coarse coding&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In principle, &lt;strong&gt;any&lt;/strong&gt; function approximator can be used, but RL has specific properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Experience is not i.i.d — successive time-step are correlated&lt;/li&gt;
  &lt;li&gt;Agent’s policy affects the data it receives&lt;/li&gt;
  &lt;li&gt;Value functions $v_\pi(s)$ can be non-stationary&lt;/li&gt;
  &lt;li&gt;Feedback is delayed, not instantaneous&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;classes-of-function-approximation&quot;&gt;Classes of Function Approximation&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Tabular: a table with an entry for each MDP state&lt;/li&gt;
  &lt;li&gt;State aggregation: Partition environment states&lt;/li&gt;
  &lt;li&gt;Linear function approximate: fixed feature (or fixed kernel)&lt;/li&gt;
  &lt;li&gt;Differentiable (nonlinear) function approximation: neural nets&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approximate-values-by-stochastic-gradient-descent&quot;&gt;Approximate Values By Stochastic Gradient Descent&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: fins $\theta$ that minimise the difference between $v_\theta(s)$ and   $v_\pi(s)$ 
&lt;script type=&quot;math/tex&quot;&gt;J(\theta)=\mathbb{E}[(v_\pi(S)-v_\theta(S))^2]&lt;/script&gt;
      Note: The expectation if over the state distribution — e.g., induced by the policy&lt;/li&gt;
  &lt;li&gt;Gradient descent:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Delta\theta=-\frac{1}{2}\alpha\nabla_\theta J(\theta)=\alpha\mathbb{E}[(v_\pi(S)-v_\theta(S))\nabla_\theta v_\theta(S)]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stochastic&lt;/strong&gt; gradient descent&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Delta\theta_t=\alpha(v_\pi(S_t)-v_\theta(S_t))\nabla_\theta v_\theta(S_t)&lt;/script&gt;
    &lt;h3 id=&quot;feature-vectors&quot;&gt;Feature Vectors&lt;/h3&gt;
    &lt;hr /&gt;
  &lt;/li&gt;
  &lt;li&gt;Represent state by a &lt;strong&gt;feature vector&lt;/strong&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\phi(s) = \left(\begin{array}{cc}
\phi_1(s) \\ \vdots \\ \phi_n(s)
\end{array}\right)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;$\phi:S\rightarrow\mathbb{R}^n$ is a fixed mapping from state (e.g. observation) to features&lt;/li&gt;
  &lt;li&gt;Short-hand: $\phi_t=\phi(S_t)$&lt;/li&gt;
  &lt;li&gt;For example:
    &lt;ul&gt;
      &lt;li&gt;Distance of robot from landmarks&lt;/li&gt;
      &lt;li&gt;Trends in the stock market&lt;/li&gt;
      &lt;li&gt;Piece and pawn configurations in chess&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approximate-values-by-stochastic-gradient-descent-1&quot;&gt;Approximate Values By Stochastic Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Goal: fina $\theta$ that minimise the difference between $v_\theta(s)$ and $v_\pi(s)$ 
&lt;script type=&quot;math/tex&quot;&gt;J(\theta)=\mathbb{E}[(v_\pi(S)-v_\theta(S))^2]&lt;/script&gt;  &lt;br /&gt;
Note: The expectation if over the state distribution — e.g., induced by the policy.&lt;/li&gt;
  &lt;li&gt;Gradient descent:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Delta\theta=-\frac{1}{2}\alpha\Delta_\theta J(\theta)=\alpha\mathbb{E}_\pi p[(v_\pi(S)-v_\theta(S))\nabla_\theta v_\theta(S)]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stochastic&lt;/strong&gt; gradient descent:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Delta\theta_t=\alpha(v_\pi(S_t)-v_\theta(S_t))\nabla_\theta v_\theta(S_t)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;linear-value-function-approximation&quot;&gt;Linear Value Function Approximation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Approximate value function by a linear combination of features&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v_\theta(s)=\theta^\top \phi(s)=\sum_{j=1}^n\phi_j(s)\theta_j&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Objective function (‘loss’) is quadratic in $\theta$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;J(\theta)=\mathbb{E}_\pi\big[(v_\pi(S)-\theta^\top\phi(S))^2\big]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Stichastic gradient descent converges on global ooptimum&lt;/li&gt;
  &lt;li&gt;Update rule is simple&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta v_\theta(S_t)=\phi(S_t)=\phi_t \quad \Longrightarrow \quad \Delta_\theta = \alpha(v_\pi(S_t)-v_\theta(S_t))\phi_t&lt;/script&gt;
      $\large \text{Update}=\textbf{step size}\times\textbf{prediction error}\times\textbf{feature vector}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;incremental-prediction-algorithms&quot;&gt;Incremental Prediction Algorithms&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;The true value function $v_\pi(s)$ is typically not available&lt;/li&gt;
  &lt;li&gt;In practice, we substitute a &lt;strong&gt;target&lt;/strong&gt; for $v_\pi(s)$
    &lt;ul&gt;
      &lt;li&gt;For MC, the target is the return $G_t$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Delta\theta_t=\alpha(G_t - v_\theta(s))\nabla_\theta v_\theta(s)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;For TD, the target is the TD target $R_{t+1}+\gamma v_\theta(S_{t+1})$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Delta\theta_t=\alpha(R_{t+1}+\gamma v_\theta(S_{t+1}) - v_\theta(S_t))\nabla_\theta v_\theta(S_t)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;monte-carlo-with-value-function-approximation&quot;&gt;Monte-Carlo with Value Function Approximation&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;The return $G_t$ is an unbiased, noisy sample of $v_\pi(s)$&lt;/li&gt;
  &lt;li&gt;Can therefore apply supervised learning to (online) “training data”&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{(S_0,G_0),\ldots,(S_t,G_t)}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For example, using &lt;strong&gt;linear Monte-Carlo policy evaluation&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\Delta\theta_t &amp;=\alpha(G_t - v_\theta(S_t))\nabla_\theta v_\theta(S_t) \\
&amp; = \alpha(G_t - v_\theta(S_t))\phi_t
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Monte-Carlo evaluation converges to a local optimum&lt;/li&gt;
  &lt;li&gt;Even when using non-linear value function approximation&lt;/li&gt;
  &lt;li&gt;For linear function, it finds the globlal optimum&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;td-learning-with-value-function-approximation&quot;&gt;TD Learning with Value Function Approximation&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;The TD-target $R_{t+1}+\gamma v_\theta(S_{t+1})$ is a &lt;strong&gt;biased&lt;/strong&gt; sample og true value $v_\pi(S_t)$&lt;/li&gt;
  &lt;li&gt;Can still apply supervised learning to “training data”&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;{(S_0,R_1+\gamma v_\theta(S_1)),\ldots,(S_t,R_{t+1}+\gamma v_\theta(S_{t+1}))}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For example, using linear TD&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\Delta\theta_t &amp;= \alpha\underbrace{(R_{t+1}+\gamma v_\theta(S_{t+1})-v_\theta(S_t))}_{\normalsize =\delta_t, \text{TD error}}\nabla_\theta v_\theta(S_t) \\
&amp; =\alpha\delta_t\phi_t
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convergence-of-mc-and-td&quot;&gt;Convergence of MC and TD&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;with linear functions, MC converges to&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\min_\theta\mathbb{E}\big[(G_t-v_\theta(S_t))^2\big]=\mathbb{E}\big[\phi_t\phi_t^\top\big]^{-1}\mathbb{E}\big[v_\pi(S_t)\phi_t\big]&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;With linear function, TD converges to&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\min_\theta\mathbb{E}\big[(R_{t+1}+\gamma v_\theta(S_{t+1}-v_\theta(S_t)))^2\big]=\mathbb{E}\big[\phi_t(\phi_t-\gamma\phi_{t+1})^\top\big]\mathbb{E}\big[R_{t+1}\phi_t\big]&lt;/script&gt;
(in continuing problem with fixed $\gamma$)&lt;/li&gt;
  &lt;li&gt;This is a different solution from MC&lt;/li&gt;
  &lt;li&gt;Typically, the asymptotic MC solution is preferred&lt;/li&gt;
  &lt;li&gt;But TD methods may converge faster,   and may still be better&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\textbf{TD:}\quad\Delta_t=\alpha\delta\nabla_\theta v_\theta(S_t)\quad \textbf{where} \quad \delta_t=R_{t+1}+\gamma v_\theta(S_{t+1}-v_\theta(S_t))&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;This update ignores dependence of $v_\theta(S_{t+1})$ on $\theta$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;action-value-function-approximation&quot;&gt;Action-Value Function Approximation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Approximate the action-value function&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;q_\theta(s,a)\approx q_\pi(S,a)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For instance, with linear function approximation&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;q_\theta(s,a)=\phi(s,a)_\top\theta=\sum_{j=1}^n\phi_j(s,a)\theta_j&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Stochastic gradient descent update&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\Delta\theta &amp;= \alpha(q_\pi(s,a)-q_\theta(s,a))\nabla_\theta q_\theta(s,a) \\
&amp;= \alpha(q_\pi(s,a)-q_\theta(s,a))\phi(s,a)
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;least-squarse-prediction&quot;&gt;Least Squarse Prediction&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Given value function approximation $v_\theta(s) \approx v_\pi(s)$&lt;/li&gt;
  &lt;li&gt;And &lt;strong&gt;experience&lt;/strong&gt; $\mathcal{D}$ consisting of $\large \langle \text{state, estimated value} \rangle$pairs&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\big\{\langle S_1,\hat{v}_1^\pi \rangle,\langle S_2,\hat{v}_2^\pi \rangle,\ldots,\langle S_T,\hat{v}_T^\pi \rangle \big\}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;E.g., $\large \hat{V}&lt;em&gt;1^\pi=R&lt;/em&gt;{t+1}+\gamma v_\theta(S_{t+1})$&lt;/li&gt;
  &lt;li&gt;Which parameters $\theta$ give the best fitting value function $v_\theta(s)$?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stochastic-gradient-descent-with-experience-replay&quot;&gt;Stochastic Gradient Descent with Experience Replay&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;Give experience consisting of $\large \langle \text{state, value} \rangle$pairs&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\big\{\langle S_1,\hat{v}_1^\pi \rangle,\langle S_2,\hat{v}_2^\pi \rangle,\ldots,\langle S_T,\hat{v}_T^\pi \rangle \big\}&lt;/script&gt;&lt;br /&gt;
Repeat:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sample state, value from experience&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\langle s, \hat{v}_\pi  \rangle \sim \mathcal{D}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Apply stochastic gradient decent update&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Delta\theta = \alpha(\hat{v}^\pi - v_\theta(s))\nabla_\theta v_\theta(s)&lt;/script&gt;&lt;br /&gt;
Converges to least squares solution&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\theta_\pi=\mathop{\text{argmin}}\limits_\theta LS(\theta)=\mathop{\text{argmin}}\limits_\theta\mathbb{E}_\mathcal{D}\big[(\hat{v}_i^\pi-v_\theta(S_i))^2\big]&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;linear-least-squares-prediction&quot;&gt;Linear Least Squares Prediction&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Experience replay finds least squares solution&lt;/li&gt;
  &lt;li&gt;But it may take many iterations&lt;/li&gt;
  &lt;li&gt;Using &lt;strong&gt;linear&lt;/strong&gt; value function approximation $v_\theta(s)=\phi(s)^\top\theta$ we can solve the least squares solution directly&lt;/li&gt;
  &lt;li&gt;At minimum of $LS(\theta)$, the expected update must be zero&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathbb{E}_\mathcal{D}[\Delta\theta] &amp;= 0 \\
\alpha\sum_{t=1}^T\phi_t(\hat{v}_t^\pi-\phi_t^\top\theta) &amp;= 0 \\
\sum_{t=1}^T\phi_t\hat{v}_t^\pi &amp;= \sum_{t=1}^T\phi_t\phi_t^\top\theta \\
\theta_t &amp;= \Big(\sum_{t=1}^T\phi_t\phi_t^\top\Big)^{-1}\sum_{t=1}^T\phi_t\hat{v}_t^\pi
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For N feature, direct solution time is $O(N^3)$&lt;/li&gt;
  &lt;li&gt;Incremental solution time is $O(N^2)$ using Shermann-Morrison&lt;/li&gt;
  &lt;li&gt;We do not know true values $v_\pi$ (have estimates $\hat{v}_t$)&lt;/li&gt;
  &lt;li&gt;In practice, our “training data” must use noisy or biased sample of $v_\pi$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;      &amp;lt;font color=blue&amp;gt;&lt;strong&gt;LSMC&lt;/strong&gt;&amp;lt;/font&amp;gt; Least Squares Monte-Carlo uses return&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v_\pi \approx G_t&lt;/script&gt;
      &amp;lt;font color=blue&amp;gt;&lt;strong&gt;LSTD&lt;/strong&gt;&amp;lt;/font&amp;gt; Least Squares Temporal-Difference uses TD target&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v_\pi \approx R_{t+1} + \gamma v_\theta(S_{t+1})&lt;/script&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In each case we can solve directly for the fixed poine&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deep-reinforcement-learning&quot;&gt;Deep reinforcement learning&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Many ideas immediately transfer when using deep neural networks:
    &lt;ul&gt;
      &lt;li&gt;TD and MC&lt;/li&gt;
      &lt;li&gt;Double learning (e.g., double Q-learning)&lt;/li&gt;
      &lt;li&gt;Experience replay&lt;/li&gt;
      &lt;li&gt;…&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Some ideas do not easily transfer
    &lt;ul&gt;
      &lt;li&gt;UCB&lt;/li&gt;
      &lt;li&gt;Least squares TD/MC&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;neural-q-learning&quot;&gt;Neural Q-learning&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Online neural Q-learning may include:
    &lt;ul&gt;
      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $q_\theta:\space O_t \Longrightarrow (q[1],\ldots,q[m])(m\space \text{actions})$&lt;/li&gt;
      &lt;li&gt;An $\epsilon-\text{greedy}$ &lt;strong&gt;exploration policy&lt;/strong&gt;: $q_t\space \Longrightarrow \space \pi_t \Longrightarrow \space A_t$&lt;/li&gt;
      &lt;li&gt;A Q-learning &lt;strong&gt;loss function&lt;/strong&gt; on $\theta$&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;I(\theta)=\frac{1}{2}\Big(R_{t+1}+\gamma\Big[\max_a q_\theta (S_{t+1},a)\Big] - q_\theta (S_t, A_t)\Big)^2&lt;/script&gt;&lt;br /&gt;
where $[\cdot ]$ denotes stopping the gradient, so that the gradient is&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta I(\theta)=\Big(R_{t+1}+\gamma\max_a q_\theta(S_{t+1},a)-q_\theta(S_t,A_t)\Big)\nabla_\theta q_\theta(S_t,A_t)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;An &lt;strong&gt;optimizer&lt;/strong&gt; to minimize the loss (e.g., SGD, RMSProp, Adma)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dqn&quot;&gt;DQN&lt;/h3&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;DQN (Mnih et al. 2013, 2015) includes;
    &lt;ul&gt;
      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $q_\theta:\space O_t \mapsto (q[1],\ldots,q[m])(m\space \text{actions})$&lt;/li&gt;
      &lt;li&gt;An $\epsilon-\text{greedy}$ &lt;strong&gt;exploration policy&lt;/strong&gt;: $q_t\space \mapsto \space \pi_t \Longrightarrow \space A_t$&lt;/li&gt;
      &lt;li&gt;A &lt;strong&gt;replay buffer&lt;/strong&gt; to store and sample past transitions&lt;/li&gt;
      &lt;li&gt;A &lt;strong&gt;target network&lt;/strong&gt; $q_{\theta^-}:\space Q_t \mapsto\space(q^-[1],\ldots,q^-[m])$&lt;/li&gt;
      &lt;li&gt;A Q-learning &lt;strong&gt;loss function&lt;/strong&gt; on $\theta$ (use replay and target network) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;I(\theta)=\frac{1}{2}\Big(R_{t+1}+\gamma\Big[\max_a q_{\theta^-} (S_{t+1},a)\Big] - q_\theta (S_t, A_t)\Big)^2&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;An &lt;strong&gt;optimizer&lt;/strong&gt; to minimize the loss (e.g., SGD, RMSProp, Adma)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Replay and target networks make RL look more like supervised learning&lt;/li&gt;
  &lt;li&gt;It is unclear whether they are vital, but they help&lt;/li&gt;
  &lt;li&gt;“DL-aware RL”&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;n-step-return&quot;&gt;n-Step Return&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Consider the following n-steps returns for $n=1,2,\infty: $&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp;n=1 \quad &amp;(TD)\quad &amp; G_T^{(1)}=R_{t+1} + \gamma v(S_{t+1}) \\
&amp;n=2 \quad &amp;\quad &amp; G_T^{(2)}=R_{t+1} + \gamma R_{t+2} +\gamma^2 v(S_{t+2}) \\
&amp;\quad \vdots \quad &amp;\quad &amp; \vdots \\
&amp;n=\infty \quad &amp;(MC)\quad &amp; G_T^{(\infty)}=R_{t+1} + \gamma R_{t+2}+\ldots+\gamma^{T-t-1}R_T
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Define the n-step return&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n v(S_{t+n})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;n-step temporal-difference learning&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v(S_t) \leftarrow v(S_t) + \alpha\Big(G_t^{(n)} - v(S_t)\Big)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 12 May 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2019/Function-Approximation/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/Function-Approximation/</guid>
        
        
      </item>
    
      <item>
        <title>Markov descision processess</title>
        <description>&lt;h3 id=&quot;introduction-to-mdps&quot;&gt;Introduction to MDPs&lt;/h3&gt;
&lt;hr /&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Markov decision processes (MDPs)&lt;/strong&gt; formally describe an environment.&lt;/li&gt;
  &lt;li&gt;Assume the environment is fully observable the current: the current observations contains relevant information.&lt;/li&gt;
  &lt;li&gt;Almost all RL problems can be formalized as MDPs, e.g,
    &lt;ul&gt;
      &lt;li&gt;Optimal control primarily deals with continuous MDPs&lt;/li&gt;
      &lt;li&gt;Partially observable problems can be converted into MDPs&lt;/li&gt;
      &lt;li&gt;Bandits are MDPs with one state&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;
&lt;p&gt;A state $s$ has the &lt;strong&gt;Markvo&lt;/strong&gt; property when for states $\forall s^{‘}\in S$ and all rewards $r \in \mathbb{R}$ 
&lt;script type=&quot;math/tex&quot;&gt;p(R_{t+1}=r,S_{t+1}=s^{'}|S_{t}=s)= 
p(R_{t+1}=r,S_{t+1}=s^{'}|S_{1},...,S_{t},S_{t-1}=s)&lt;/script&gt;&lt;br /&gt;
for all possible histories $S_{1},…,S_{t-1}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The state capture all relevent information from history&lt;/li&gt;
  &lt;li&gt;Once the state is know, the history may be throw away&lt;/li&gt;
  &lt;li&gt;The state is a sufficient statistic of the past&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;return&quot;&gt;Return&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Acting in a MDP results in &lt;strong&gt;return&lt;/strong&gt; $G_{t}$: total discounted reward from time-step $t$&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;G_{t}=R_{t+1}+\gamma R_{t+2}+ ... = \sum_{k=0}^{\infty}\gamma_{k}R_{t+k+1}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;This is a random variables that depends on &lt;strong&gt;MDP&lt;/strong&gt; and &lt;strong&gt;policy&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;discount&lt;/strong&gt; $\gamma\in[0,1]$ is the present value of future rewards
    &lt;ul&gt;
      &lt;li&gt;The marginal value of receiving reward $R$ after $k+1$time-steps is $\gamma^{k}R$&lt;/li&gt;
      &lt;li&gt;For $\gamma&amp;lt;1$, immediate rewards are more important than delayed rewards&lt;/li&gt;
      &lt;li&gt;$\gamma$ close to 0 leads to “myopic” evaluation&lt;/li&gt;
      &lt;li&gt;$\gamma$ close to 1 leads to “far-sighted” evaluation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;value-function&quot;&gt;Value Function&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;The value function $v(s)$ gives the long-term value of state $s$ &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}(s)=\mathbb{E}[G_{t}|S_{t}=s,\pi]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;It can be defined recursively&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
  v_{\pi}(s) &amp;=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_{t}, \pi] \\
  &amp;=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s, A_{t}\sim\pi(S_{t})] \\
  &amp;=\sum_{a}\pi(a|s)\sum_{r}\sum_{s^{'}}p(r,s^{'}|s,a)(r+\gamma v_{\pi}(s^{'}))
  \end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;The final step writes out the expectation explicitly&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;action-values&quot;&gt;Action Values&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;We can define state-action values  &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;q_{\pi}(s,a)=\mathbb{E}[G_{t}|S_{t}=s,A_{t}=a,\pi]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This implies: the value of a state is equal to the weighted sum of the state action value by definition&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
  q_{\pi}(s,a)&amp;=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s, A_{t}=a] \\
  &amp;=\mathbb{E}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a] \\
  &amp;=\sum_{r}\sum_{s^{'}}p(r,s^{'}|s,a)\Big(r+\gamma\sum_{a^{'}}\pi(a^{'},s^{'})q_{\pi}(s^{'},a^{'})\Big)
  \end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Note that &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
  v_{\pi}(s) &amp; =  \sum_{a}\pi(a|s)q_{\pi}(s,a) \\ 
  &amp; = \mathbb{E}[q_{\pi}(S_{t},A_{t})|S_{t}=s,\pi], \forall s
  \end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;tips&quot;&gt;Tips&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Estimating $v_{\pi}$ or $q_{\pi}$ is called &lt;strong&gt;policy evaluation&lt;/strong&gt; or, simply, &lt;strong&gt;prediction&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Estimating $v_{*}\space \text{or}\space  q_{*}$ is sometimes called &lt;strong&gt;control&lt;/strong&gt;, because these can be used for &lt;strong&gt;policy optimizaton&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;bellman-equation&quot;&gt;Bellman Equation&lt;/h3&gt;
&lt;p&gt;Four Bellman equations:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
v_{\pi}(s) &amp;=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s,A_{t}\sim\pi(S_{t})] \\
v_{*}(s) &amp;=\max_{a}\mathbb{E}[R_{t+1}+\gamma v_{*}(S_{t+1})|S_{t}=s,A_{t}=a] \\
q_{\pi}(s,a) &amp;=\mathbb{E}[R_{t+1}+\gamma q_{\pi}(S_{t+1}, A_{t+1})|S_{t}=s,A_{t}=a] \\
q_{*}(s,a) &amp;=\mathbb{E}[R_{t+1}+\gamma \max_{a^{'}} q_{*}(S_{t+1},a^{'})|S_{t}=s,A_{t}=a]
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;### Policy Evaluate&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We start by discussing how to estimate&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;v_{\pi}(s)=\mathbb[R_{t+1}=\gamma v_{\pi}(S_{t+1})|s,\pi]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Idea: turn this equality into an update&lt;/li&gt;
  &lt;li&gt;First, initialize $v_{0}$ e.g. to zero.&lt;/li&gt;
  &lt;li&gt;Then iterate&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;\forall s: v_{k+1}(s)=\mathbb{E}[R_{t+1}=\gamma v_{k}(S_{t+1})|s,\pi]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note: whenever $v_{k+1}(s)=v_{k}(s)$, for all $s$, we must have found $v_{\pi}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;This policy evaluation is always converge under appropriate conditions (e.g., $\gamma &amp;lt; 1$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Implies $\lim_{k\rightarrow\infty}v_{k}=v_{\pi}$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Finite-horizon episodic case is a bit harder, but also works&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 03 May 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2019/Markov-Descision-Processess/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/Markov-Descision-Processess/</guid>
        
        
      </item>
    
  </channel>
</rss>
