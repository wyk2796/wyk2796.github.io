<!DOCTYPE html>
<html lang="en" class="no-js">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
   <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
  <head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.55.6" />

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">

<link rel="alternate" type="application/rss&#43;xml" href="/docs/index.xml">

<link rel="shortcut icon" href="/assets/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/assets/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/assets/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/assets/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/assets/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/assets/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/assets/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/assets/favicons/android-96x196.png" sizes="96x196">
<link rel="icon" type="image/png" href="/assets/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/assets/favicons/android-192x192.png"sizes="192x192">

<title>Search</title>
<meta property="og:title" content="Search" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:4000" />
<meta property="og:site_name" content="http://localhost:4000" />

<meta itemprop="name" content="Search">
<meta itemprop="description" content="">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Search"/>
<meta name="twitter:description" content=""/>

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="/assets/css/palette.css">
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous">
</script>


    <script type="text/x-mathjax-config"> 
      MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); 
    </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({tex2jax: {
            inlineMath: [ ['$','$'] ],
            processEscapes: true
          }
        });
   </script>
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
   </script>
</head>

  

  <body class="td-section">
    <header>
        <nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
            <span class="navbar-logo"></span><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149v0 0zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.328-2.2733-15.458-6.4032s-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zM197.0804 232.033c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zM197.0839 232.0372c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zM197.0839 232.0372c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><path style="fill:#5b7fc0" d="M198.8952 225.1043h122.6266v13.8671H198.8952z"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.328-2.2733-15.458-6.4032s-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zM197.0804 177.6188c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zM197.0839 177.623c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zM197.0839 177.623c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><path style="fill:#d95140" d="M198.8952 170.69h122.6266v13.8671H198.8952z"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zM197.5309 286.4723c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zM197.5344 286.4765c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zM197.5344 286.4765c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><path style="fill:#56a55c" d="M199.3456 279.5436h122.6266v13.8671H199.3456z"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.328-2.2733-15.458-6.4032-4.13-4.1299-6.4032-9.6186-6.4056-15.4628.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zM197.0804 340.5784c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zM197.0839 340.5826c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zM197.0839 340.5826c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><path style="fill:#f1bc42" d="M198.8952 333.6497h122.6266v13.8671H198.8952z"/></g></g></svg>
<span class="text-uppercase font-weight-bold">Yukai Wu</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			<li class="nav-item mr-4 mb-2 mb-lg-0">
                            <a class="nav-link" href="https://github.com/wyk2796" target="_blank"><span>GitHub</span></a>
			</li>
			<li class="nav-item mr-4 mb-2 mb-lg-0">
                            <a class="nav-link" href="/about" ><span>About</span></a>
			</li>
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block">
 <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
        </div>

	<div class="navbar-nav d-none d-lg-block">
            <!-- <div class="gh-source__repository"> -->
    <a class="gh-source" data-gh-source="github" href="https://github.com/wyk2796" title="Go to repository" data-md-state="done">
      <i class="fab fa fa-github fa-2x" style='padding-right:20px; float:left; margin-top:5px'></i>
    </a>
    <a  class="gh-source"   data-gh-source="github"href="https://www.linkedin.com/in/yukau-wu" title="Go to linkedin" data-md-state="done">
      <i class="fab fa-linkedin-in fa-2x" style='padding-right:20px; float:left; margin-top:5px'></i>
    </a>
              <!-- yukai wu/yukai wu -->
              <!-- </div> -->
            <!-- </div> -->
              <!-- <ul class="gh-source__facts">
                <li class="gh-source__fact" id='stars'>
                </li>
                <li id="forks" class="gh-source__fact">
                </li>
              </ul> -->
  </div>

</nav>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$']]}
                            });
  </script>
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</header>


<script>
$(document).ready(function() {
  var url = "https://api.github.com/search/repositories?q=yukai wu/yukai wu";
  fetch(url, { 
      headers: {"Accept":"application/vnd.github.preview"}
  }).then(function(e) {
    return e.json()
  }).then(function(r) {
     console.log(r.items[0])
     stars = r.items[0]['stargazers_count']
     forks = r.items[0]['forks_count']
     $('#stars').text(stars + " Stars")
     $('#forks').text(forks + " Forks")
  });
});
</script>

    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          <div id="td-sidebar-menu" class="td-sidebar__inner">  
  <form class="td-sidebar__search d-flex align-items-center">
 <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>  
  <nav class="collapse td-sidebar-nav pt-2 pl-4" id="td-section-nav">
<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">List</a>
  </li><ul>
    <li class="collapse show" id="list">
        <ul class="td-sidebar-nav__section pr-md-3">
          <li class="td-sidebar-nav__section-title">
            <a href="/news" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Study Note</a>
          </li>
          <ul><li class="collapse show" id=""></li></ul>
          <li class="td-sidebar-nav__section-title">
            <a href="/about" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">About Me</a>
          </li>
          <ul><li class="collapse show" id=""></li></ul>
    </ul>
  </nav>
</div>

          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
              <div class="td-page-meta ml-2 pb-1 pt-2 mb-0">
                  <a href="https://github.com/wyk2796/edit/master/pages/search.html" target="_blank"><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/wyk2796/issues/new?labels=question&title=Question:&body=Question on: https://github.com/wyk2796/tree/master/pages/search.html" target="_blank"><i class="fab fa-github fa-fw"></i> Create documentation issue</a>
<a href="https://github.com/wyk2796/issues/new" target="_blank"><i class="fas fa-tasks fa-fw"></i> Create project issue</a>
<!-- this will parse through the header fields and add a button to open
     an issue / ask a question on Github. The editable field should be in
     the post frontend matter, and refer to the label to open the issue for -->

              </div>
              <nav id="TableOfContents"><ul>
              <li><ul id="TOC">
                <!-- Links will be appended here-->
              </ul></li>
              </ul></nav>
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            <nav aria-label="breadcrumb" class="d-none d-md-block d-print-none">
	      <ol class="breadcrumb spb-1">
                <li class="breadcrumb-item active" aria-current="page">
	          <a href="/search/">Search</a>
                </li>
	      </ol>
           </nav>
           <div class="td-content">
	      <input class="form-control td-search-input" type="search" name="q" id="search-input" placeholder="&#xf002 Search this site…"  style="margin-top:5px" autofocus>
<i style="color:white; margin-right:8px; margin-left:5px" class="fa fa-search"></i>

<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>

<ul id="search-results"></ul>

<script>
	window.data = {
		
				
					
					
					"docs-example-page": {
						"id": "docs-example-page",
						"title": "A Nested Page",
						"categories": "",
						"url": " /docs/example-page",
						"content": "A Nested Page\n\nThis is an example of a page that doesn’t have a permalink defined, and\nis not included in the table of contents (_data/toc.yml). This means\nthat it will render based on it’s path. Since it’s in docs/example-page.md,\nthe url will be docs/example-page/.\n\nLink to a subfolder\n\nNow let’s say we want to link to a subfolder, specifically with this\nsetup:\n\ndocs/\n  example-page.md  (-- we are here\n  subfolder/\n     example-page.md  (-- we want to link here\n\n\nYou can provide the relative path to the file, like subfolder/example-page.md\nand Jekyll will handle parsing it. For example:\n\n\n  here is that link\n\n\nAnd here\n is the same link, \nbut generated with the include statement:\n\n{% include doc.html name=\"here\" path=\"subfolder/example-page\" %}"
					}
					
				
		
				
					,
					
					"docs-extras-example-quiz": {
						"id": "docs-extras-example-quiz",
						"title": "Quiz",
						"categories": "",
						"url": " /docs/extras/example-quiz",
						"content": "Quizzes\n\nAs of version 0.0.12, mkdocs-jekyll has support for basic quizzes! These are\nintended to help educate your users about the content of your documentation.\nFor a quiz, you can add a new file to the folder _data/quizzes, and write a \nquestions file based on the format shown in _data/quizzes/example-quiz.yml.\nHere is a simple example of a multiple choice question (which can also serve as \nTrue/False):\n\ntitle: This is the Quiz Title\nrandomized: false\nquestions:\n\n - type: \"multiple-choice\"\n   question: \"True or False, Pittsburgh is West of Philadelphia\"\n   items:\n    - choice: True\n      correct: true\n    - choice: False\n      correct: false\n   followup: | \n      The answer is True! Pittsburgh is 304.9 miles West of \n      Philadelphia, or approximately a car ride of \n      4 hours and 52 minutes. Buckle up!\n\n\nThe quiz is rendered with a “Show Answer” button below each question, and when\nthe user clicks it, any questions that are flagged with correct: true will be \nbolded, and if a followup section is included, it will be displayed.\nSee the live example at the end of this page.\n\nOptions\n\nTitle\n\nIf you include a title, it will be rendered at the top of the quiz. This is\noptional - you can leave it out and add it before the include on the page.\n\nRandom\n\nIf you want your questions to be presented randomly, just add randomized: true\nto the data.\n\nExample Quiz\n\nIf I want to include the quiz located at _data/quizzes/example-quiz.yml, I \ncan do so like this:\n\n{% include quiz.html file='example-quiz' %}\n\n\nThe rendered quiz is shown here:\n\nThis is the Quiz Title\n\nWhat is your favorite color?\n1. Red2. Blue3. Green\n\nShow Answer\nThere is no correct answer to asking your favorite color! All choices would be good.\n\n\nTrue or False, Pittsburgh is West of Philadelphia\n1. true2. false\n\nShow Answer\nThe answer is True! Pittsburgh is 304.9 miles West of Philadelphia, or approximately \na car ride of 4 hours and 52 minutes. Buckle up!"
					}
					
				
		
				
					,
					
					"docs-extras-index": {
						"id": "docs-extras-index",
						"title": "Extras",
						"categories": "",
						"url": " /docs/extras/index",
						"content": "Extras\n\nExtras include other integrations that aren’t relevant to style or customization,\nbut can further enhance your documentation pages. Currently, we have support\nfor adding interactive quizzes.\n\n\n  Quizzes\n\n\nWould you like to see another question type, or another kind of extra? Please\n[open an issue])(https://github.com/wyk2796/issues/new)."
					}
					
				
		
				
					,
					
					"docs-getting-started": {
						"id": "docs-getting-started",
						"title": "Getting Started",
						"categories": "",
						"url": " /docs/getting-started",
						"content": "Getting Started\n\nFeatures\n\nUser Interaction\n\nOn the right side of any page, you’ll notice links to edit the page, or\nopen an issue. This ensures that any time you have a question or want to \nsuggest or request a change, you can do so immediately and link directly\nto the section of interest. The sections on the page also have permalinks so\nyou can link directly to them.\n\nSearch\n\nThe entire site, including posts and documentation, is indexed and then available\nfor search at the top or side of the page. Give it a try! The content is rendered\ninto window data that is used by lunr.js to generate the search results.\nIf you want to exclude any file from search, add this to its front end matter:\n\n---\nlayout: null\nexcluded_in_search: true\n---\n\n\nThe example above is for a javascript file in the assets folder that is used as a template,\nbut should not be included in search.\n\nExternal Search\n\nIf you have an external site with a search GET endpoint (meaning one that ends\nin ?q=&lt;term&gt;, then you can automatically link page tags to search this endpoint.\nFor example, on an HPC site I’d want a tag like “mpi” to do a search on \nhttp://ask.cyberinfrastructure.org for mpi.\nSee the tags section below for how to configure this.\n\nDocumentation\n\nDocumentation pages should be written in the docs folder of the repository,\nand you are allowed to use whatever level of nesting (subfolders) that \nworks for you! It’s a Jekyll collection, which means that you\ncan add other content (images, scripts) and it will be included for linking to.\n\nOrganization\n\nThe url that will render is based on the path. For example, if we had the following structure:\n\ndocs/\n  getting-started.md\n  clusters/\n     sherlock/\n         getting-started.md\n\n\nThe first page (akin to the one you are reading) would render at it’s path,\n/docs/getting-started/.\n\nLinking\n\nFrom that page, we could provide the\ndirect path in markdown to any subfolder to link to it, such as the second\ngetting started page for sherlock:\n\n[example](clusters/sherlock/getting-started.md)\n\n\nHere is an example link to a relative path of a file (example-page.md)\nin the same directory, and from that page you can test linking to a subfoldr.\nIn the case of not having a subfolder, we could write the link out directly:\n\n[example]({{ site.baseurl }}/docs/clusters/sherlock/getting-started.md)\n\n\nor just put the relative path:\n\n[Here](example-page)\n\n\nor better, there is a shortand trick! We can use the provided “includes” \ntemplate to do the same based on the path to create a link:\n\n{% include doc.html name=\"Sherlock Cluster\" path=\"clusters/sherlock/getting-started\" %}\n\nThe path should be relative to the docs folder.\n\nPages\n\nThe pages folder uses the same page layout, but is not part of the docs collection.\nThe two are provided to create a distinction between website pages (e.g., about,\nfeed.xml) and documentation pages.\n\nNavigation\n\nWhether you place your page under “pages” or “docs,” for those pages that you want added to the navigation, \nyou should add them to _data/toc.yml. If you’ve defined a permalink in the\nfront end matter, you can use that (e.g., “About” below). If you haven’t and\nwant to link to docs, the url is the path starting with the docs folder.\nHere is an example (currently the active example):\n\n- title: Documentation\n  url: docs\n  links:\n    - title: \"Getting Started\"\n      url: \"docs/getting-started\"\n      children:\n        - title: Features\n          url: \"docs/getting-started#getting-started\"\n        - title: Development\n          url: \"docs/getting-started#development\"\n        - title: Customization\n          url: \"docs/getting-started#customization\"\n    - title: \"Extras\"\n      url: \"docs/extras\"\n      children:\n        - title: Quizzes\n          url: \"docs/extras/example-quiz\"\n    - title: \"About\"\n      url: \"about\"\n    - title: \"News\"\n      url: \"news\n\n\nIf you want to add an external url for a parent or child, do this:\n\n  - title: GitHub Repository\n    external_url: https://www.github.com/vsoch/mkdocs-jekyll\n\n\nNews Posts\n\nIt might be the case that your site or group has news items that would\nwarrent sharing with the community, and should be available as a feed.\nFor this reason, you can write traditional posts in the _posts\nfolder that will parse into the site feed\nThe bottom of the page links the user to a post archive, where posts are organized\naccording to the year.\n\nButtons\n\nButtons come in a nice array of colors. Here is the code for a basic example,\nand you’d want to vary the .btn-&lt;tag&gt; to get different classes.\n\n&lt;button class=\"btn btn-success\"&gt;.btn-success&lt;/button&gt;\n\n\n.btn-success\n.btn-info\n.btn-secondary\n.btn-primary\n.btn-danger\n.btn-warning\n\nBadges\n\nFor news post items, it’s nice to be able to tag it with something that indicates\na status, such as “warning” or “alert.” For this reason, you can add badges to\nthe front end matter of any post page, and they will render colored by a type,\nwith the tag of your choice. For example, here is an example header for\na post:\n\n---\ntitle:  \"Two Thousand Nineteen\"\ndate:   2019-06-28 18:52:21\ncategories: jekyll update\nbadges:\n - type: warning\n   tag: warning-badge\n - type: danger\n   tag: danger-badge\n---\n\n\nAnd here is the post preview with the rendered badges that it produces:\n\nwarning-badge\ndanger-badge\n\nAnd the other badges that you can define include success, info, secondary,\nand primary.\n\nsuccess-badge\ninfo-badge\nsecondary-badge\nprimary-badge\n\nAlerts\n\n\nWhat is an alert?\nAn alert is a box that can stand out to indicate important information. You can choose from levels success, warning, danger, info, and primary. This example is an info box, and the code for another might look like this:\n\n\n{% include alert.html type=\"info\" title=\"Here is another!\" %}\n\n\nJust for fun, here are all the types:\n\n\nwarning\nThis is a warning\n\n\n\ndanger\nThis alerts danger!\n\n\n\nsuccess\nThis alerts success\n\n\n\ninfo\nThis is useful information.\n\n\n\nprimary\nThis is a primary alert\n\n\n\nsecondary\nThis is a secondary alert\n\n\nQuotes\n\nYou can include block quotes to emphasize text.\n\n\n  Here is an example. Isn’t this much more prominent to the user?\n\n\nDevelopment\n\nInitially (on OS X), you will need to setup Brew which is a package manager for OS X and Git. To install Brew and Git, run the following commands:\n\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nbrew install git\n\n\nIf you are on Debian/Ubuntu, then you can easily install git with apt-get\n\napt-get update &amp;&amp; apt-get install -y git\n\n\nInstall Jekyll\n\nYou can also install Jekyll with brew.\n\n$ brew install ruby\n$ gem install jekyll\n$ gem install bundler\n$ bundle install\n\n\nOn Ubuntu I do a different method:\n\ngit clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build\necho 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' &gt;&gt; ~/.bashrc\nexec $SHELL\nrbenv install 2.3.1\nrbenv global 2.3.1\ngem install bundler\nrbenv rehash\nruby -v\n\n# Rails\ncurl -sL https://deb.nodesource.com/setup_4.x | sudo -E bash -\nsudo apt-get install -y nodejs\ngem install rails -v 4.2.6\nrbenv rehash\n\n# Jekyll\ngem install jekyll\ngem install github-pages\ngem install jekyll-sass-converter\n\nrbenv rehash\n\n\nGet the code\n\nYou should first fork the repository to your GitHub organization or username,\nand then clone it.\n\n$ git clone https://github.com/&lt;username&lt;/mkdocs-jekyll.git docs\n$ cd docs\n\n\nYou can clone the repository right to where you want to host the docs:\n\n$ git clone https://github.com/&lt;username&gt;/mkdocs-jekyll.git docs\n$ cd docs\n\n\nServe\n\nDepending on how you installed jekyll:\n\njekyll serve\n# or\nbundle exec jekyll serve\n\n\nPreview\n\nWe provide a CircleCI configuration recipe that you\ncan use to preview your site on CircleCI before merging into master. You\nshould follow the instructions to set up a project,\nand then in the project settings be sure to enable building forked build requests,\nand to cancel redundant builds. The preview will be built on CircleCI, and saved\nto static files for you to browse. The only change you will need is to edit\nthe static files location to be the name of your respository, which is at te\nbottom of the .circleci/config.yml file:\n\n      - store_artifacts:\n          path: ~/repo/_site\n          destination: mkdocs-jekyll\n\n\nIn the above, the destination should coincide with your repository name.\nRemember that for most links, CircleCI won’t honor an index.html file in a subfolder\n(e.g., subfolder/index.html will not be served as subfolder/, so for example,\nyou might need to turn this:\n\nhttps://&lt;circleci&gt;/0/mkdocs-jekyll/docs/getting-started/\n\ninto this:\n\nhttps://&lt;circleci&gt;/0/mkdocs-jekyll/docs/getting-started/index.html\n\n\nCustomization\n\nconfig.yml\n\nTo edit configuration values, customize the _config.yml.\nMost are documented there, and please open an issue if you have questions.\n\nAdding pages\n\nTo add pages, write them into the pages folder. \nYou define urls based on the permalink attribute in your pages,\nand then add them to the navigation by adding to the content of _data/toc.yml.\n\nTags\n\nIf you include tags on a page, by default they will link to the tags page on the site. However, if you define a tag_search_endpoint url in your configuration file, by clicking\nthe tag, the user will be taken to this page to search for it. As an example,\nwe define the current search endpoint to be Ask Cyberinfrastructure, and\npage tags link to a search on it:\n\ntag_search_endpoint: https://ask.cyberinfrastructure.org/search?q=\ntag_color: danger # danger, success, warning, primary, secondary, info\n\n\nNote that you can also choose a color! The tags appear at the top of the page,\nas they do on this page. The tags should be defined like this in the front end\nmatter:\n\ntags: \n - jekyll\n - github\n\n\nThey are appended to the first h1 block, so generally your pages should have a header.\nIf you comment out this variable, then each of your tags will link to it’s appropriate\nspot on the tags page linked above.\n\n#tag_search_endpoint: https://ask.cyberinfrastructure.org/search?q=\ntag_color: primary # danger, success, warning, primary, info, secondary"
					}
					
				
		
				
					,
					
					"docs-subfolder-example-page": {
						"id": "docs-subfolder-example-page",
						"title": "A Nested Page",
						"categories": "",
						"url": " /docs/subfolder/example-page",
						"content": "A Nested Page\n\nThis is an example of a page that doesn’t have a permalink defined, and\nis not included in the table of contents (_data/toc.yml)."
					}
					
				
		
		
				
					,
					
					"about": {
						"id": "about",
						"title": "About Me",
						"categories": "",
						"url": " /about/",
						"content": "YuKai Wu\nemail: ywu048@fiu.edu \ncall: 7866083426\n\nEDUCATION\n\n  2009.09—2013.07 AnQing Normal University (bachelor degree), major Information and Computing Science\n  2018.05—2019.12 Florida International University (master degree), major computer science\n\n\nPROFESSIONAL EXPERIENCE\n\n  \n    2015.9 – 2018.1   HangZhou ShuYun Information Science Corporation (Algorithm Engineer)\n    \n      Design and Development of Recommendation System for commodities\nThe company started to develop a framework for Recommendation System which can train model from users features data and generates a list of recommendation commodities. In this project, My duty is to develop a back-end part that can get data from several recommendation lists and merge them by specific algorithm, finally push them to user end. I’m also in charge with developing some recommendation algorithms for some particular scenarios.  This recommendation framework mainly serves the advertising business.\n      Design and Development of TaoBao, TMall User Portrait System\nDepending on user shopping records, I establish a label system, representing user feature space, for all of them, such as Preference for Clothes, Preference for Cosmetics, Preference for Food etc. It is advantageous to divide group users into several categories and improve the precise of recommendation system.\n      Analysis user comment data with NLP\nThe company needs to extract short labels from user comments to help our customers fully understand what the user feel about their product. I use RNN with attention model to extract the key information in the short comment, analyse these data and display the results.\n    \n  \n  \n    2015.6 – 2015.8\t   ShangHai YiChen Information Science Corporation (Big Data Development Engineer)\n    \n      Design and Development of Recommendation System for short videos\nThe company started to establish recommendation system. what I do is storing user’s history data in Hive, training the Recommendation Models with Spark and storing those models in HDFS. I develop a framework that automatically deploys the trained models that had been loaded in HDFS. Recommending short videos for users and putting the results into Hbase are my next work. The frame has RESTful API for responding the request from other front-end programme.\n    \n  \n  \n    2014.5 – 2015.5\t    ANHUI XiangXing Information Science Corporation (Data Mining Engineer)\n    \n      Development of Data Mining System Based on Spark Framework\nThe company wants to create a cloud computing platform for the enterprise that is beneficial for them to analyse huge amount of data. Therefore, my task is to integrate Spark into this System and extend the function and algorithm of MLlib.\n      Data Modeling of Customer Loss with Anhui Telecom User Data\nHelp our customer to build a ML model to predict whether the user would lost or not in the next mouth.  The Telecom company gives me lots of user behavior data without sensitive part.  The label is binary, meaning whether the user would lost in next month. I use previous four-month data to extract the user’s feature, and then input the features into model to predict the user’s situation in next month.\n    \n  \n\n\nRESEARCH / PROJECT EXPERIENCE\n\n  2019 Reinforcement Learning Project: (Git-Repo) :\n    \n      Navigation: Training an agent to learn how to get maximum bananas in one episode,  using the Deep Q-Network(DQN) Algorithm.\n      Reacher: Training a robot arm to quickly catch a target object. In this project, I use the Deep Deterministic Policy Gradient(DDPG) with continuous action space to train 20 virtual agents simultaneously to reduce the training time.\n      Tennis: Two players compete with each other. The project is training two agents to play the tennis game. I use the DDPG to train multi-agents in competitive environment.\n      Half Field Offense in Robocup 2D Soccer: Training an agent to kick the ball to the goal. This environment action space is parameterized action space which combines the discrete value and the continuous value.\n    \n  \n  \n    2018 NLP Project (Git-Repo, Paper) :\nWe use four kinds of algorithms to complete Name Entity Recognition(NER) task and compare them.\n  \n  \n    2019 Image Process (Git-Repo, Report) :\nFor this project, My target is to display the intermediate processes of image-style transformation, how to transform a photo from a original photo to a style photo and what the processes look like. I use VGG19 and transform-net (Johnson, J. et al. 2016) to train model for image-style transformation. In each layer of transform-net, I use DeconvNet to get visible image from intermediate data to display the changing process.\n  \n  \n    2019 Distributed Framework:\nIt is a distributed system framework and has a master and various plumbable components. The plumbable components can be designed to divers function components. The master manages all components. The system has a pub-sub system to support communication between all components. The component can publish and subscribe topics. This project is based on AKKA concurrency framework with Scala programm language. Here is a Distributed Web Crawler based on this distributed framework,  “crawlnet” (Git-Repo).\n  \n  2015 Recommendation System (Git-Repo) :\nThe project target is to design and develop a recommendation system to recommend commodity for users. The architecture of the project includes four parts. I use Hive as my data warehouse that stores all orders data, user and commodities data. I use Spark to generate models used to recommend commodities and store the result into Hbase database. And then we design restful API to receive requests and return the recommendation commodities for users. The algorithm I use in this project is user-item interactions matrix, Collaborative filtering, and TF-IDF etc.\n\n\nPROFESSIONAL SKILLS\n\n  Deep learning development skills:\n    \n      Tensorflow, pyTorch and relevant Python lib, Such as Numpy, Pandas etc.\n      Reinforcement learning:\n        \n          Get the Nanodegree in Udacity online course.\n          MDP, DQN, DDPG, PPO, A2C,A3C, .\n        \n      \n      NLP\n        \n          word2vec, RNN, LSTM, seq2seq, and attention model.\n        \n      \n      Image Process\n        \n          CNN, VGG, ResNet, DeconvNet, Image Style_Transfer\n        \n      \n    \n  \n  Big Data development skills:\n    \n      Spark distributed computing system, experience for dealing with large-scale data sets and the spark-mllib machine learning algorithm library.\n      Recommendation System:\n        \n          Framework design and development\n          Design Recommendation Algorithm\n          Design User Portrait\n        \n      \n      AKKA concurrency framework, and Real-time stream processing with Spark.\n      Big Data related systems, such as: Hadoop, Spark, Hive, Hbase, Kafka, Elasticsearch etc.\n      Relational database Mysql and non-relational database, such as : HBase, Mongodb and Redis\n    \n  \n  Development Skills:\n    \n      Programming language Scala, Python, Java, C/C++, SQL, etc.\n      Tools: git, vscode, vs, JetBrains, sbt, Maven, CMake,   Amazon Web Services(AWS)\n    \n  \n\n\nCERTIFICATE\nUdacity Nanodegree:\n\nWEBSITE\nGithub, Linkedin, Personal Website"
					}
					
				
		
				
					,
					
					"archive": {
						"id": "archive",
						"title": "Article",
						"categories": "",
						"url": " /archive/",
						"content": "Articles\n\n2019\n\n\n  May 23, 2019: Planning and models\n  \n\n\n\n  May 17, 2019: Policy gradients and actor critic\n  \n\n\n\n  May 12, 2019: Function approximation\n  \n\n\n\n  May 3, 2019: Markov descision processess"
					}
					
				
		
				
					,
					
					"docs": {
						"id": "docs",
						"title": "Documentation",
						"categories": "",
						"url": " /docs/",
						"content": "Documentation\n\nWelcome to the Yukai Wu Documentation pages! Here you can quickly jump to a \nparticular page.\n\n\n    \n            \n    \n    A Nested Page\n    An example of a nested page\n            \n    \n    Quiz\n    How to add interactive quizzes to your site.\n            \n    \n    Extras\n    Extras, including quizzes.\n            \n    \n    Getting Started\n    Getting started with Docsy Jekyll\n            \n    \n    A Nested Page\n    An example of a a nested page in a subfolder."
					}
					
				
		
				
					,
					
					"feed-xml": {
						"id": "feed-xml",
						"title": "",
						"categories": "",
						"url": " /feed.xml",
						"content": "Yukai Wu\n    \n    http://localhost:4000/\n    \n    Wed, 22 Jan 2020 22:29:46 -0500\n    Wed, 22 Jan 2020 22:29:46 -0500\n    Jekyll v3.8.5\n    \n      \n        Planning and models\n        &lt;h3 id=&quot;1-model-based-and-model-free-rl&quot;&gt;1. Model-Based and Model-Free RL&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Model-Free RL\n    &lt;ul&gt;\n      &lt;li&gt;No model&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; value function (and/or policy) from experience&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Model-Based RL\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; a model from experience OR be given a model&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; value function (and/or policy) from model&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;!--more--&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;2-what-is-a-model&quot;&gt;2. What is a Model?&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;A model $\\mathcal{M}_{\\eta}\\space\\text{is a representation of MDP}\\space\\langle\\mathcal{S,A},\\hat{p}_{\\eta}\\rangle$&lt;/li&gt;\n  &lt;li&gt;$\\space\\langle\\mathcal{S, A},\\hat{p}_\\eta\\rangle$&lt;/li&gt;\n  &lt;li&gt;For now, we will assume the states and actions are the same as in the real problem&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;The model &lt;strong&gt;approximates&lt;/strong&gt; the state transitions and rewards $\\hat{p}_\\eta\\approx p$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;R_{t+1}, S_{t+1} \\sim \\hat{p}_{\\eta} (r, s'|S_t,A_t)&lt;/script&gt;&lt;br /&gt;\n(Note there is not probability distribution function)&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Optionally, we could model rewards and state dynamics separately&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;3-model-learning&quot;&gt;3. Model Learning&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Goal: &lt;strong&gt;estimate&lt;/strong&gt; model $\\mathcal{M}_\\eta$ from experienve ${S_1, A_1, R_2,\\ldots,S_T}$&lt;/li&gt;\n  &lt;li&gt;This is a supervised learning problem&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\nS_1,A_1 &amp;\\rightarrow R_2, S_2 \\\\\n&amp;\\vdots \\\\\nS_{T-1},A_{T-1} &amp;\\rightarrow R_{T}, S_{T}\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Learn a function $f(s,a) = r,s’$&lt;/li&gt;\n  &lt;li&gt;Pick loss function (e.g. mean-squared error), and find parameters $\\eta$ that minimise empirical loss&lt;/li&gt;\n  &lt;li&gt;This would give an expectation model&lt;/li&gt;\n  &lt;li&gt;If $f(s,a)=r,s’$, then we would hope $s’\\approx \\mathbb{E}[S_{t+1}|s=S_t,a=A_t]$&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;4-stochastic-model&quot;&gt;4. Stochastic Model&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;We may not want to assume everything is linear&lt;/li&gt;\n  &lt;li&gt;Then expected states may not be roght — they may not correspond to actual states, and iterating the model may do weird things.&lt;/li&gt;\n  &lt;li&gt;Alternative: &lt;strong&gt;stochastic models&lt;/strong&gt; (also know as &lt;strong&gt;generative models&lt;/strong&gt;) \n&lt;script type=&quot;math/tex&quot;&gt;\\hat{R}_{t+1}, \\hat{S}_{t+1} = \\hat{p}(S_t, A_t,\\omega)&lt;/script&gt;\nwhere $\\omega$ is a noise term&lt;/li&gt;\n  &lt;li&gt;Stochastic models can be chained, even if the model is non-linear.&lt;/li&gt;\n  &lt;li&gt;But they do add not noise&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;5-exmples-of-models&quot;&gt;5. Exmples of Models&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Table Looking Model&lt;/li&gt;\n  &lt;li&gt;Linear Expectation Model&lt;/li&gt;\n  &lt;li&gt;Linear Gaussian Model&lt;/li&gt;\n  &lt;li&gt;Deep Neural Network Model&lt;/li&gt;\n  &lt;li&gt;……&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;51-table-lookup-model&quot;&gt;5.1 Table Lookup Model&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;Model is an explicit MDP&lt;/li&gt;\n  &lt;li&gt;Count visits $N(s,a)$ to each state action pair&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n\\hat{p}_t(s'|s,a) &amp;= \\frac{1}{N(s,a)}\\sum_{k=0}^{t-1}I(S_k=s,A_k=a,S_{k+1}=s') \\\\\n\\mathbb{\\hat{p}_t}[R_{t+1}|S_t=s,A_t=a] &amp;= \\frac{1}{N(s,a)}\\sum_{k=0}^{t-1}I(S_k=s,A_k=a)R_{k+1}\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Alternatively, use non-parameteric ‘replay’\n    &lt;ul&gt;\n      &lt;li&gt;At each time-step t, record experience tuple $\\langle S_t,A_t,R_{t+1},S_{t+1} \\rangle$&lt;/li&gt;\n      &lt;li&gt;To sample model, randomly pick tuple matching $\\langle s,a,\\cdot,\\cdot\\rangle$&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;6-planing-with-a-model&quot;&gt;6. Planing with a Model&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Given a model $\\hat{p}_\\eta$&lt;/li&gt;\n  &lt;li&gt;Solve the MDP $\\langle \\mathcal{S,A},\\hat{p}_\\eta \\rangle$&lt;/li&gt;\n  &lt;li&gt;Using favourite planning algorithm\n    &lt;ul&gt;\n      &lt;li&gt;Value iteration&lt;/li&gt;\n      &lt;li&gt;Policy iteration&lt;/li&gt;\n      &lt;li&gt;Tree search&lt;/li&gt;\n      &lt;li&gt;……&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;61-sample-based-planning&quot;&gt;6.1 Sample-Based Planning&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;A simple but powerful approach to planning&lt;/li&gt;\n  &lt;li&gt;Use the model &lt;strong&gt;only&lt;/strong&gt; to generate sampler&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Sample&lt;/strong&gt; experience from model&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;S,R \\sim \\hat{p}_\\eta(\\cdot|s,a)&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Apply &lt;strong&gt;model-free&lt;/strong&gt; RL to sample, e.g.:\n    &lt;ul&gt;\n      &lt;li&gt;Monte-Carlo control&lt;/li&gt;\n      &lt;li&gt;Sarsa&lt;/li&gt;\n      &lt;li&gt;Q-learning&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;7-conventional-model-based-and-model-free-metheds&quot;&gt;7. Conventional model-based and model-free metheds&lt;/h3&gt;\n\n&lt;p&gt;Traditional RL algorithms did not explicitly store their experiences, and were often placed into one of two groups.&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;Model-free&lt;/strong&gt; methods update the value function and/or policy and do not have explicit dynamics models.&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Models-based&lt;/strong&gt; methods update the transition and reward models, and compute a value function or policy from the model.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;8-using-experience-in-the-place-of-model&quot;&gt;8. Using experience in the place of model&lt;/h3&gt;\n&lt;p&gt;Recall prioritized sweeping from tabular dynamic programming&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;Update the value function of the states with the largest magnitude Bellman errors using a priority queue.\nA related idea is prioritized experience replay (Schaul et al, 2015) which works from experience for general function approximation.&lt;/li&gt;\n  &lt;li&gt;The experience replay buffer maintain a priority for each transition, with the priority given by the magnitude of the Bellman error.&lt;/li&gt;\n  &lt;li&gt;Minibatches are sampled using this priority to quickly reduce errors.&lt;/li&gt;\n  &lt;li&gt;Weighted importance sampling corrects for bias from non-uniform sampling&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;9-limits-of-planning-with-an-inaccurate-model&quot;&gt;9. Limits of Planning with an Inaccurate Model&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Given an imperfect model $\\hat{p}_\\eta \\neq p$&lt;/li&gt;\n  &lt;li&gt;Performance is limited to optimal policy for approximate MDP $\\langle \\mathcal{M,A,\\hat{p}_\\eta}\\rangle$&lt;/li&gt;\n  &lt;li&gt;Model-based RL is only as good as the estimated model&lt;/li&gt;\n  &lt;li&gt;When the model is inaccurate, planning process will compute a suboptimal policy (not covered in these slides)\n    &lt;ul&gt;\n      &lt;li&gt;Approach 1: when model is wrong, use model-free RL&lt;/li&gt;\n      &lt;li&gt;Approach 2: reson explicitly about model uncertainty over $\\eta$ (e.g. Bayesian methon)&lt;/li&gt;\n      &lt;li&gt;Approach 3: Combine model-based and model-free methods in a safe way.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;10-real-and-simulated-experience&quot;&gt;10. Real and Simulated Experience&lt;/h3&gt;\n&lt;p&gt;We consider two sources of experience\nReal experience Sampled from environment (true MDP)&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;r,s'\\sim p&lt;/script&gt;&lt;br /&gt;\nSumulated experience Sampled from model (approximate MDP)&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;r,s' \\sim \\hat{p}_\\eta&lt;/script&gt;&lt;/p&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;11-intergrating-learning-and-planning&quot;&gt;11. Intergrating Learning and Planning&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Model-Free RL\n    &lt;ul&gt;\n      &lt;li&gt;No model&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; value function (and/or policy) from real experience&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Model-Based RL (using Sample-Based Planning)\n    &lt;ul&gt;\n      &lt;li&gt;Learn a model from real experience&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; value function (and/or policy) from simulated experience&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Dyna\n    &lt;ul&gt;\n      &lt;li&gt;Learn a model from real experience&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Learn AND plan&lt;/strong&gt; value function (and/or policy) from real and simulated experience&lt;/li&gt;\n      &lt;li&gt;Treat real and sumulated esperience equivalently. Conceptually, the update from learning or planning are not distinguished.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;111-dyna-q-algorithm&quot;&gt;11.1 Dyna-Q Algorithm&lt;/h4&gt;\n\n&lt;blockquote&gt;\n  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n&amp; \\text{Initialize}\\space Q(s,a)\\space \\text{and Model}(s,a)\\quad \\text{for all}\\space s\\in\\mathcal{A}(s) \\\\\n&amp; \\text{Do forever:} \\\\\n&amp; \\qquad (a) \\quad s \\leftarrow \\text{current (nonterminal) state} \\\\\n&amp; \\qquad (b)\\quad a \\leftarrow \\epsilon\\text{-greedy}(s,Q) \\\\\n&amp; \\qquad (c) \\quad \\text{Execute action}\\space a\\text{; observe resultant state, }s'\\text{, and reward, }r\\\\\n&amp; \\qquad (d)\\quad Q(s,a)\\leftarrow Q(s,a)+\\alpha[r + \\gamma\\max_{a'}Q(s',a')-Q(s,a)]\\\\\n&amp; \\qquad (e)\\quad Model(s,a) \\leftarrow s',r \\quad\\text{(assuming deterministic environment)}\\\\\n&amp; \\qquad (f)\\quad \\text{Repeat}\\space N \\space \\text{times:} \\\\\n&amp; \\qquad\\qquad s \\leftarrow \\text{random previously observed state} \\\\\n&amp; \\qquad\\qquad a\\leftarrow \\text{random action previously taken in}\\space s \\\\\n&amp; \\qquad\\qquad s',r \\leftarrow Model(s,a) \\\\\n&amp; \\qquad\\qquad Q(s,a)\\leftarrow Q(s,a) + \\alpha[r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)]\n\\end{aligned} %]]&gt;&lt;/script&gt;\n&lt;/blockquote&gt;\n\n&lt;h3 id=&quot;112-dyna-with-function-approximation&quot;&gt;11.2 Dyna with Function Approximation&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;How can an agent plan when the actual environmental states are not know?&lt;/li&gt;\n  &lt;li&gt;Can directly approximate probability distributions of the transitions and the rewards.&lt;/li&gt;\n  &lt;li&gt;Probability distribution models in high dimensional feature spaces are computationally expensive and often inaccurate!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;12-planning-for-action-selection&quot;&gt;12. Planning for Action Selection&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;We considered the case where planning is used to improve a global value function&lt;/li&gt;\n  &lt;li&gt;Now consider planning for the near future, to select the next action&lt;/li&gt;\n  &lt;li&gt;The distribution of states that may be encounted from &lt;strong&gt;now&lt;/strong&gt; can diff from the distribution of states encountered from a starting state&lt;/li&gt;\n  &lt;li&gt;The agent may be able to make a more accurate local value function (for the states that will be encountered soon) than the global value function&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;121-forward-search&quot;&gt;12.1 Forward Search&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;Forward search&lt;/strong&gt; algorithms select the best action by &lt;strong&gt;look ahead&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;They build a search tree with the current state $s_t$ at the root&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;Using a &lt;strong&gt;model&lt;/strong&gt; of the MDP to look ahead\n&lt;img src=&quot;https://i.loli.net/2019/12/24/QkTADdw2EPx4zyG.png&quot; alt=&quot;tree&quot; /&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;No need to solve whole MDP, just sub-MDP starting from &lt;strong&gt;now&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;122-simulation-based-search&quot;&gt;12.2 Simulation-Based Search&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;Forward&lt;/strong&gt; search paradigm using sample-based planning&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Simulate&lt;/strong&gt; episodes of experience from &lt;strong&gt;now&lt;/strong&gt; with the model&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;Apply &lt;strong&gt;model-free&lt;/strong&gt; RL to simulated episodes&lt;br /&gt;\n&lt;img src=&quot;https://i.loli.net/2019/12/24/JmliqxjnY7TB5fI.png&quot; alt=&quot;Search Path&quot; /&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Simulate episode of experience from &lt;strong&gt;now&lt;/strong&gt; with the model&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\{S_t^k, A_t^k,R_{t+1}^k,\\ldots,S_T^k\\}_{k=1}^K \\sim \\hat{p}_\\eta&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Apply &lt;strong&gt;model-free&lt;/strong&gt; RL to sumulated episodes\n    &lt;ul&gt;\n      &lt;li&gt;Monte-carlo control $\\rightarrow$ Monte-Carlo search&lt;/li&gt;\n      &lt;li&gt;Sarsa $\\rightarrow$ TD search&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;123-search-tree-vs-value-function-approximation&quot;&gt;12.3 Search tree vs. value function approximation&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;Search tree is a table lookup approach&lt;/li&gt;\n  &lt;li&gt;Based on a partial instantiation of the table&lt;/li&gt;\n  &lt;li&gt;For model-free reinforcement learning, table lookup is naive\n    &lt;ul&gt;\n      &lt;li&gt;Can’t store value for all states&lt;/li&gt;\n      &lt;li&gt;Doesn’t generalise between similar states&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;For simulation-based search, table lookup is less naive\n    &lt;ul&gt;\n      &lt;li&gt;Search tree stores value for easily reachable states&lt;/li&gt;\n      &lt;li&gt;In huge search spaces, value function approximation is helpful&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;13-monte-carlo-simulation&quot;&gt;13. Monte-Carlo Simulation&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Given a parameterized model $\\mathcal{M}_\\eta$ and a &lt;strong&gt;simulation policy&lt;/strong&gt; $\\pi$&lt;/li&gt;\n  &lt;li&gt;Simulate $K$ episodes from current state $S_t$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\{S_t^k=S_t,A_t^k,R_{t+1}^k,S_{t+1}^k,\\ldots,S_t^k\\}_{k=1}^K \\sim \\hat{p}_\\eta, \\pi&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Evaluate state by mean return (&lt;strong&gt;Monte-Carlo evaluaiton&lt;/strong&gt;)&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;v(\\color{red}{S_t})=\\frac{1}{K}\\sum_{k=1}^K G_t^k \\rightsquigarrow v_\\pi(S_t)&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;131-simple-monte-carlo-search&quot;&gt;13.1 Simple Monte-Carlo Search&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;Given a model $\\mathcal{M}_\\eta$ and a policy $\\pi$&lt;/li&gt;\n  &lt;li&gt;For each action $a \\in \\mathcal{A}$\n    &lt;ul&gt;\n      &lt;li&gt;Simulate $K$ episodes from current (real) state $s$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\{S_t^k=s,A_t^k=a,R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,\\ldots,S_t^k\\}_{k=1}^K \\sim \\mathcal{M}_v,\\pi&lt;/script&gt;&lt;/li&gt;\n      &lt;li&gt;Evaluate actions by mean return (Monto-Carlo evaluation)&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;q(\\color{red}{s,a})=\\frac{1}{K}\\sum_{k=1}^K G_t^k \\rightsquigarrow q_\\pi(S_t)&lt;/script&gt;&lt;/li&gt;\n      &lt;li&gt;Select current (real) action with maximum value&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;A_t=\\mathop{\\text{argmax}}\\limits_{a\\in\\mathcal{A}}q(S_t,a)&lt;/script&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;132-monte-carlo-tree-search-evaluation&quot;&gt;13.2 Monte-Carlo Tree Search (Evaluation)&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;Given a model $\\mathcal{M}_\\eta$&lt;/li&gt;\n  &lt;li&gt;Simulate $K$ episodes from current state $S_t$ using current simulation policy $\\pi$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\{\\color{red}{S_t^k=S_t},A_t^k,R_{t+1}^k,S_{T+1}^k,\\ldots,S_t^k\\}_{k=1}^K \\sim \\mathcal{M}_v,\\pi&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Build a search tree containing visited states and actions&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Evaluate&lt;/strong&gt; states $q(s,a)$ by mean return of eqisodes from $s,a$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;q(s,a)=\\frac{1}{N(s,a)}\\sum_{k=1}^K\\sum_{u=t}^T1(S_u^k,A_u^k=s,a)(G_u^k \\rightsquigarrow q_\\pi(s,a))&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;After searching, select current (real) action with maximum value in search tree&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;a_t=\\mathop{\\text{argmax}}\\limits_{a\\in\\mathcal{A}}q(S_t,a)&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h4 id=&quot;monte-carlo-tree-search-simulation&quot;&gt;Monte-Carlo Tree Search (Simulation)&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;In MCTS, the simulation policy $\\pi$ &lt;strong&gt;improves&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;The simulation policy $\\pi$ has two phases (in-tree, out-of-tree)\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;Tree policy&lt;/strong&gt; (improves): pick action from $q(s,a)$ (e.g. $\\epsilon$-greedy($q(s,a)$))&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Rollout policy&lt;/strong&gt; (fixed): e.g., pick actions randomly&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Repeat (for each simulated episode)\n    &lt;ul&gt;\n      &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt; actions in tree according to tree policy&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Expand&lt;/strong&gt; search tree by one node&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Rollout&lt;/strong&gt; to termination with default policy&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Update&lt;/strong&gt; action-values $q(s,a)$ in the tree&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Output best action when simulation time runs out.&lt;/li&gt;\n  &lt;li&gt;With some asumptions, converges to the optimal values, $q(s,a)\\Rightarrow q_*(s,a)$&lt;/li&gt;\n&lt;/ul&gt;\n\n        Thu, 23 May 2019 00:00:00 -0400\n        http://localhost:4000/2019/Planning-and-Models/\n        http://localhost:4000/2019/Planning-and-Models/\n        \n        \n      \n    \n      \n        Policy gradients and actor critic\n        &lt;h3 id=&quot;general-overview&quot;&gt;General Overview&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Model-based RL:&lt;br /&gt;\n + ‘Easy’ to learn a model (supervised learning) &lt;br /&gt;\n + Learns ‘all there is to know’ from the data&lt;br /&gt;\n - Objective capture irrelevant information&lt;br /&gt;\n - May focus compute/capacity on irrelevant detail&lt;br /&gt;\n - Computing policy (planning) is non-trivial and can be computationally expensive&lt;/li&gt;\n  &lt;li&gt;Valued-based RL:&lt;br /&gt;\n + Closer to true objective\n + Fairly well-understood — somewhat similar to regression&lt;br /&gt;\n - Still not the true objective — may still focus capacity on less-important details&lt;/li&gt;\n  &lt;li&gt;Policy-based RL:&lt;br /&gt;\n + Right objective!\n - Ignores other learnable knowledge (potentially not the most efficient use of data)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;!--more--&gt;\n&lt;hr /&gt;\n&lt;h3 id=&quot;advantage-of-policy-based-rl&quot;&gt;Advantage of Policy-Based RL&lt;/h3&gt;\n&lt;p&gt;Advantages:&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;Good convergence properties&lt;/li&gt;\n  &lt;li&gt;Easily extended to high-dimensional or continuous action spaces&lt;/li&gt;\n  &lt;li&gt;Can learn &lt;strong&gt;stochastic&lt;/strong&gt; policies&lt;/li&gt;\n  &lt;li&gt;Sometimes Policies are &lt;strong&gt;simple&lt;/strong&gt; while values and models are complex\n    &lt;ul&gt;\n      &lt;li&gt;E.g., rich domain, but optimal is always go left.&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Disadvantages:&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;Susceptible to local optimal (expecially with non-linear FA)&lt;/li&gt;\n  &lt;li&gt;Obtained knowledge is specific, does not always generalize well&lt;/li&gt;\n  &lt;li&gt;Ignores a lot of information in the data (when used in isolation)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;policy-objective-functions&quot;&gt;Policy Objective Functions&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Goal: given policy $\\pi_{\\theta}(s,a)$ with patameters $\\theta$, find best $\\theta$&lt;/li&gt;\n  &lt;li&gt;But how do we measure the quality of a policy $\\pi_{\\theta}$?&lt;/li&gt;\n  &lt;li&gt;In episodic environments we can use the &lt;strong&gt;start value&lt;/strong&gt;&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;J_{1}(\\theta)=v_{\\pi_{\\theta}}(s_{1})&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;In continuing environments we can use the average value&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;J_{avV}(\\theta)=\\sum_{s}\\mu_{\\pi_{\\theta}}(s)v_{\\pi_{\\theta}}(s)&lt;/script&gt;&lt;br /&gt;\nwhere $\\mu_{\\pi}(s)=p(S_{t}=s|\\pi)$ is the probability of being in state s in the long run\nThink of is as the ratio of time spent in $s$ under policy $\\pi$&lt;/li&gt;\n  &lt;li&gt;Or the average reward per time-step&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;J_{avR}(\\theta)=\\sum_{s}\\mu_{\\pi_{\\theta}}(s)\\sum_{a}\\pi_{\\theta}(s,a)\\sum_{r}p(r|s,a)r&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;gradients-on-parameterized-policies&quot;&gt;Gradients on Parameterized Policies&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;We need to compute an estimate of the policy gradient&lt;/li&gt;\n  &lt;li&gt;Assume policy $\\pi_{\\theta}$ is differentiable almost everywhere\n    &lt;ul&gt;\n      &lt;li&gt;E.g., $\\pi_{\\theta}$is a linear function of the agent state, or a neural network&lt;/li&gt;\n      &lt;li&gt;Or we could have a parameterized class of controllers&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Goal is to compute&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\nabla_{\\theta}J(\\theta)=\\nabla\\mathbb{E}_{d}[v_{\\pi_{\\theta}}(s)]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;We will use Monte Carlo samples to compute this gradient&lt;/li&gt;\n  &lt;li&gt;So, how does $\\mathbb{E}_{d}[v_{\\pi_{\\theta}}(S)]$ depend on $\\theta$?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h3&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;The policy gradient approach also applies to (multi-step) MDPs&lt;/li&gt;\n  &lt;li&gt;Replaces instantaneous reward R with long-term value $q_{\\pi}(s,a)$&lt;/li&gt;\n  &lt;li&gt;Policy gradient theorem applies to start state objective, average reward and average value objective&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;\nFor any differentiable policy $\\pi_{\\theta}(s,a)$, for any of the policy objective functions $J=J_{1}, J_{avR}, or 1\\frac{1}{1-\\gamma}J_{avV}$, the policy gradient is&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\nabla_{\\theta}J(\\theta) = \\mathbb{E}[q_{\\pi_{\\theta}}(S,A)\\nabla_{\\theta}\\log\\pi_{\\theta}(A|S)]&lt;/script&gt;&lt;br /&gt;\nExpectation is over both states and actions&lt;/p&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;policy-gradients-on-trajectories-derivation&quot;&gt;Policy Gradients on trajectories: Derivation&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Consider trajectory $\\zeta=S_0,A_0,R_0,S_1,A_1,R_1,S_2,\\ldots$ with return $G(\\zeta)$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n&amp; \\nabla_\\theta J_\\theta(\\pi)=\\nabla_\\theta\\mathbb{E}[G(\\zeta)]=\\mathbb{E}[G(\\zeta)\\nabla_\\theta\\log p(\\zeta)]\\qquad \\text{(score function trick)} \\\\\n&amp; \\nabla_\\theta\\log p(\\zeta) \\\\\n&amp; =\\nabla_\\theta\\log\\Big[p(S_0)\\pi(A_0|S_0)p(S_1|S_0,A_0)\\pi(A_1|S_1)\\cdots)\\Big] \\\\\n&amp; =\\nabla_\\theta\\Big[\\log p(S_0)+\\log\\pi(A_0|S_0)+\\log p(S_1|S_0,A_0)+\\log\\pi(A_1|S_1)+\\cdots)\\Big] \\\\\n&amp; =\\nabla_\\theta\\Big[\\log\\pi(A_0|S_0)+\\log\\pi(A_1|S_1)+\\cdots\\Big] \\\\\n\\textbf{So:} \\\\\n&amp; \\nabla_\\theta J_\\theta(\\pi)=\\mathbb{E}\\Big[G(\\zeta)\\nabla_\\theta\\sum_{t=0}\\log\\pi(A _t|S_t)\\Big] = \\mathbb{E}\\Big[\\Big(\\sum_{t=0}R_{t+1}\\Big)\\Big(\\nabla_\\theta\\sum_{t=0}\\log\\pi(A_t|S_t)\\Big)\\Big]\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;policy-gradients-on-trajectories-reduce-variance&quot;&gt;Policy gradients on trajectories: reduce variance&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Note that, in general &lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n\\mathbb{E}[b\\nabla_{\\theta}\\log\\pi(A_{t}|S_{t})]&amp;=\\mathbb{E}\\Bigg[\\sum_{a}\\pi(a|S_{t})b\\nabla_{\\theta}\\log\\pi(a|S_{t})\\Bigg] \\\\\n&amp;=\\mathbb{E}\\Bigg[b\\nabla_{\\theta}\\sum_{a}\\pi(a|S_{t})\\Bigg] \\\\\n&amp;=\\mathbb{b}[b\\nabla_{\\theta}1] \\\\\n&amp;=0\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;The sum of probability distribution is 1&lt;/li&gt;\n  &lt;li&gt;This holds only if $b$ does not depend on the action (though it can depend on the state)&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;Implies we can subtract a &lt;strong&gt;baseline&lt;/strong&gt; to reduce variance&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;Consider trajactory $\\zeta=S_{0},A_{0},R_{0},S_{1},A_{1},R_{1},S_{2},\\ldots$ with return $G{\\zeta}$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\nabla_{\\theta}J_{\\theta}(\\pi)=\\mathbb{E}\\Bigg[\\Bigg(\\sum_{t=0}R_{t+1}\\Bigg)\\Bigg(\\nabla_{\\theta}\\sum_{t=0}log\\pi(A_{t}|S_{t})\\Bigg)\\Bigg]&lt;/script&gt;  &lt;br /&gt;\nbut $\\sum_{t=0}^{k}R_{t+1}$ does not depend on actions $A_{k+1}, A_{k+2},\\cdots,$so&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n&amp;=\\mathbb{E}\\Bigg[\\sum_{t=0}\\nabla_{\\theta}\\log\\pi(A_{t}|S_{t})\\sum_{i=0}R_{i+1}\\Bigg] \\\\\n&amp;=\\mathbb{E}\\Bigg[\\sum_{t=0}\\nabla_{\\theta}\\log\\pi(A_{t}|S_{t})\\sum_{i=\\color{red}{t}}R_{i+1}\\Bigg] \\\\\n&amp;=\\mathbb{E}\\Bigg[\\sum_{t=0}\\nabla_{\\theta}\\log\\pi(A_{t}|S_{t})q_{\\pi}(S_{t},A_{t})\\Bigg]\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;A good baseline is $v_{\\pi}(S_{t})$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\nabla_{\\theta}J_{\\theta}(\\pi)=\\mathbb{E}\\Bigg[\\sum_{t=0}\\nabla_{\\theta}\\log\\pi(A_{t}|S_{t})q_{\\pi}(S_{t},A_{t})-v_{\\pi}(S_{t})\\Bigg]&lt;/script&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Typically, we estimate $v_{w}(s)$ explicitly, and sample&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;q_{\\pi}(S_{t},A_{t})\\approx G_{t}^{(n)}&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;For instance, $G_{t}^{(1)}=R_{t+1}+ \\gamma v_{w}(S_{s+1})$&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;estimating-the-action-value-function&quot;&gt;Estimating the Action-Value Function&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;The Critic is solving a familiar problem: policy evaluation&lt;/li&gt;\n  &lt;li&gt;What is the value of policy $\\pi_{\\theta}$ for current parameters $\\pi$&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;actor-critic&quot;&gt;Actor-Critic&lt;/h3&gt;\n&lt;p&gt;&lt;strong&gt;Critic&lt;/strong&gt; Update parameters $w$ of $v_{w}$ by n-step TD (e.g., $n=1$)\n&lt;strong&gt;Actor&lt;/strong&gt; Update $\\theta$ by policy gradient&lt;/p&gt;\n\n&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n&amp; \\textbf{function}\\space\\text{ADVANTAGE ACTOR CRITIC} \\\\\n&amp; \\qquad\\text{Initialise}\\quad s, \\theta \\\\\n&amp; \\qquad\\textbf{for}\\quad t=0,1,2,\\ldots\\space \\textbf{do} \\\\\n&amp; \\qquad\\qquad \\text{Sample}\\space A_t \\sim \\pi_\\theta(S_t) \\\\\n&amp; \\qquad\\qquad \\text{Sample}\\space R_{t+1}\\space\\text{and}\\space S_{t+1} \\\\\n&amp; \\qquad\\qquad \\delta_t=R_{t+1}+\\gamma v_w(S_{t+1})-v_w(S_t) \\qquad \\text{[one-step TD-error, or \\color{red}{advantage}]}\\\\\n&amp; \\qquad\\qquad w\\leftarrow w+\\beta\\delta_t\\nabla_wv_w(S_t) \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad \\text{[TD(0)]} \\\\\n&amp; \\qquad\\qquad \\theta\\leftarrow\\theta+\\alpha\\delta_t\\nabla_\\theta\\log\\pi_\\theta(A_t|S_t) \\qquad\\qquad\\qquad\\quad\\space\\space\\thinspace\\text{[Policy gradient update]}\\\\\n&amp; \\qquad\\textbf{end for} \\\\\n&amp; \\textbf{end function}\n\\end{aligned} %]]&gt;&lt;/script&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;full-advantage-actor-critic-agent&quot;&gt;Full Advantage Actor Critic Agent&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Adventage actor critic include\n    &lt;ul&gt;\n      &lt;li&gt;A &lt;strong&gt;representation&lt;/strong&gt; (e.g., LSTM): $(S_{t-1}, O_t)\\rightarrow S_t$&lt;/li&gt;\n      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $v_w: \\space S\\rightarrow v$&lt;/li&gt;\n      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $\\pi_\\theta:\\space S\\rightarrow \\pi$&lt;/li&gt;\n      &lt;li&gt;Copies/variants $\\pi^m$ of $\\pi_\\theta$ to use as &lt;strong&gt;policies&lt;/strong&gt;: $S_t^m\\rightarrow A_t^m$&lt;/li&gt;\n      &lt;li&gt;A n-step TD &lt;strong&gt;loss&lt;/strong&gt; on $v_w$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;I(w)=\\frac{1}{2}\\Big(G_t^{(n)}-v_w(S_t)\\Big)^2&lt;/script&gt;&lt;br /&gt;\nwhere $G_t^{(n)} = R_{t+1}+\\gamma R_{t+2}+\\ldots+\\gamma^{n-1}v_w(S_{t+n})$&lt;/li&gt;\n      &lt;li&gt;A n-step REINFORCE &lt;strong&gt;loss&lt;/strong&gt; on $\\pi_\\theta$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;I(\\theta)=\\Big[G_t^{(n)}-v_w(S_t)\\Big]\\log\\pi_\\theta(A_t|S_t)&lt;/script&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Optimizers&lt;/strong&gt; to minimize the losses&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Also know as A2C, or A3C (when combined with asynchronous parameter update)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;bias-in-actor-critic-algorithm&quot;&gt;Bias in Actor-Critic Algorithm&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Approximating the policy gradient introduce bias&lt;/li&gt;\n  &lt;li&gt;A biased policy gradient may not find the right solution&lt;/li&gt;\n  &lt;li&gt;Full return: high variance&lt;/li&gt;\n  &lt;li&gt;One-step TD-error: high bias&lt;/li&gt;\n  &lt;li&gt;n-step TD-error: useful middle ground&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n\\delta_t^{(n)} &amp;= G_t^{(n)} - v_w(S_t) \\\\\n&amp;=\\underbrace{R_{t+1} + \\gamma R_{t+2}+ \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^{n}v_w(S_t+n)}_{=G_t^{(n)}}-v_w(S_t)\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;It is really important to use close-to on-policy targets&lt;/li&gt;\n  &lt;li&gt;If needed, use importance sampling to correct&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;G_t^{(n),\\rho} = \\frac{\\pi_\\theta(A_t|S_t)}{b(A_t|S_t)}\\Big(R_{t+1}=\\gamma_{t+1}^{(n-1),\\rho}\\Big)&lt;/script&gt;&lt;br /&gt;\nwith&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;G_t^{(0),\\rho} =v_w(S_t) \\approx v_\\pi(S_t)&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h4 id=&quot;large-colordarklambdatextbf-returns&quot;&gt;$\\Large \\color{dark}{\\lambda}\\textbf{-returns}$&lt;/h4&gt;\n&lt;ul&gt;\n  &lt;li&gt;We can write a multi-step return recursively&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n&amp; G_t^{(n)}=R_{t+1}+\\gamma G_{t+1}^{n-1} \\\\\n&amp; G_t^{(0)} = v_w(S_t) \\approx v_\\pi(S_t)\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;This is equivalent to &lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;G_t^\\lambda = R_{t+1} + \\gamma(1- \\lambda_{t+1})v_w(S_{t+1}) + \\gamma\\lambda_{t+1}G_{t+1}^\\lambda&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;We can generalize to $\\lambda_t \\in [0,1]$; this is called a $\\lambda-\\text{return}$&lt;/li&gt;\n  &lt;li&gt;It can be interpreted as a &lt;strong&gt;mixture of n-step returns&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;One way to correct for off-policy returns: bootstrap (set $\\lambda=0$) whenever the policies differ&lt;/li&gt;\n  &lt;li&gt;Can be used for policy-gradient and value prediction&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;trust-region-policy-optimization&quot;&gt;Trust Region Policy Optimization&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Many extensions and variants exist&lt;/li&gt;\n  &lt;li&gt;Important: be careful with update: a bad policy leads to bad data&lt;/li&gt;\n  &lt;li&gt;This is different from supervised learning (where learning and data are independent)&lt;/li&gt;\n  &lt;li&gt;One solution: regularise policy to not change too much&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;increasing-robustness-with-trust-regions&quot;&gt;Increasing Robustness with Trust Regions&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;One way to prevent instability is to &lt;strong&gt;regularise&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;A popular method is to &lt;strong&gt;limit the difference between subsequent policies&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;For instance, use the Kullbeck-Leibler divergence:&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;KL(\\pi_{old}\\|\\pi_\\theta)=\\mathbb{E}\\Big[\\int\\pi_{old}(a|S)\\log\\frac{\\pi_\\theta(a|S)}{\\pi_{old}(a|S)}da\\Big]&lt;/script&gt;&lt;br /&gt;\n(a divergence is like a distance — but between distributions)&lt;/li&gt;\n  &lt;li&gt;Then maximise $J(\\theta)-\\eta KL(\\pi_{old}|\\pi_\\theta)$, for some small $\\eta$&lt;/li&gt;\n  &lt;li&gt;It can also help to use large batches \\\n c.f &lt;strong&gt;TRPO&lt;/strong&gt; (Schulman et al. 2015) and &lt;strong&gt;PPO&lt;/strong&gt; (Abbeel &amp;amp; Schulman 2016)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;gaussian-policy&quot;&gt;Gaussian POlicy&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;In conitnuous action spaces, a Gaussian policy is common&lt;/li&gt;\n  &lt;li&gt;E.g., mean is some function of state $\\mu(s)$&lt;/li&gt;\n  &lt;li&gt;For simplicity, lets consider fixed variance of $\\sigma^2$ (can be parameterized as well, instead)&lt;/li&gt;\n  &lt;li&gt;Policy is Gaussian, $a \\sim \\mathcal{N}(\\mu(s),\\sigma^2)$&lt;/li&gt;\n  &lt;li&gt;The gradient of the log of the policy is then&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\nabla_\\theta\\log\\pi_\\theta(s,a)=\\frac{a-\\mu(s)}{\\sigma_2}\\nabla\\mu(s)&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;This can be used, for instance, in REINFORCE / advandage actor critic&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;continuous-actor-critic-learning-automaton-cacla&quot;&gt;Continuous Actor-Critic Learning Automaton （Cacla）&lt;/h3&gt;\n\n&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n&amp; \\blacktriangleright a_t=Actor_\\theta(S_t)\\qquad  &amp;\\text{(get current (continuous) action proposal)}&amp;\\\\\n&amp; \\blacktriangleright A_t\\sim\\pi(\\cdot|S_t,a_t)\\space(e.g.,A_t \\sim \\mathcal{N}P(a_t, \\sum) \\qquad &amp;\\text{(explore)}&amp;\\\\\n&amp; \\blacktriangleright \\delta_t=R_{t+1}+\\gamma v_w(S_{t})\\qquad &amp;\\text{(compute TD error)} &amp;\\\\\n&amp; \\blacktriangleright \\text{Update}\\space v_w{S_t}\\space(\\text{e.g.,using TD}) \\qquad&amp;\\text{(policy evaluation)} &amp;\\\\\n&amp; \\blacktriangleright \\text{If}\\space \\delta_t &gt; 0, \\text{update Actor}_\\theta(S_t)\\space\\text{towards}\\space A_t \\qquad&amp;\\text{(policy improvement)}&amp;\\\\\n&amp; \\blacktriangleright \\text{if}\\space \\delta_t \\leqslant 0, \\text{do not update Actor}_\\theta \\qquad&amp;\\text{} &amp; \n\\end{aligned} %]]&gt;&lt;/script&gt;\n\n        Fri, 17 May 2019 00:00:00 -0400\n        http://localhost:4000/2019/Policy-Gradients-and-Actor-Critic/\n        http://localhost:4000/2019/Policy-Gradients-and-Actor-Critic/\n        \n        \n      \n    \n      \n        Function approximation\n        &lt;h3 id=&quot;introduce&quot;&gt;Introduce&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;The &lt;strong&gt;policy&lt;/strong&gt;, &lt;strong&gt;value function&lt;/strong&gt; and &lt;strong&gt;model&lt;/strong&gt; are all functions&lt;/li&gt;\n  &lt;li&gt;We want to learn (one of) these from experience&lt;/li&gt;\n  &lt;li&gt;If there are too many states, we need to approximate&lt;/li&gt;\n  &lt;li&gt;In general, this is called RL with function approximation&lt;/li&gt;\n  &lt;li&gt;When using deep neural nets, this is often called deep reinforcement learning&lt;/li&gt;\n  &lt;li&gt;The term is fairly new — the combination is decades old&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;!--more--&gt;\n&lt;h3 id=&quot;value-function-approximation&quot;&gt;Value Function Approximation&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;&lt;strong&gt;lookup tables&lt;/strong&gt;\n    &lt;ul&gt;\n      &lt;li&gt;Every state $s$ has an entry $q(s,a)$&lt;/li&gt;\n      &lt;li&gt;Or every state-action pair $s,a$ has an entry $q(s,a)$&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Large MDPs:\n    &lt;ul&gt;\n      &lt;li&gt;There are too many states and/or actions to store in memory&lt;/li&gt;\n      &lt;li&gt;It is too slow to learn the value of each state individually&lt;/li&gt;\n      &lt;li&gt;Individual states are often &lt;strong&gt;not fully observable&lt;/strong&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Solution for large MDPs:\n    &lt;ul&gt;\n      &lt;li&gt;Estimate value function with &lt;strong&gt;function approximation&lt;/strong&gt;&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\nv_\\theta(s)\\approx v_\\pi(s) \\qquad &amp;(or, v_*(s)) \\\\\nq_\\theta(s,a)\\approx q_\\pi(s,a) \\qquad &amp;(or,q_*(s,a))\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Generalise&lt;/strong&gt; from seen states to unseen states&lt;/li&gt;\n      &lt;li&gt;&lt;strong&gt;Update&lt;/strong&gt; parameter $\\theta$ using MC or TD observable&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;If the environement state is not fully observable:\n    &lt;ul&gt;\n      &lt;li&gt;Use the &lt;strong&gt;agent state&lt;/strong&gt;&lt;/li&gt;\n      &lt;li&gt;Consider learning a &lt;strong&gt;state update function&lt;/strong&gt; $S_{t+1}=u(S_t,O_{t+1})$&lt;/li&gt;\n      &lt;li&gt;Henceforth, $S_t$ denotes the agent state&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;which-function-approximator&quot;&gt;Which Function Approximator?&lt;/h3&gt;\n&lt;p&gt;There are many function approximators, e.g.&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;Artificial neural network&lt;/li&gt;\n  &lt;li&gt;Decision tree&lt;/li&gt;\n  &lt;li&gt;Nearest neighbour&lt;/li&gt;\n  &lt;li&gt;Fourier / wavelet bases&lt;/li&gt;\n  &lt;li&gt;Coarse coding&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In principle, &lt;strong&gt;any&lt;/strong&gt; function approximator can be used, but RL has specific properties:&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;Experience is not i.i.d — successive time-step are correlated&lt;/li&gt;\n  &lt;li&gt;Agent’s policy affects the data it receives&lt;/li&gt;\n  &lt;li&gt;Value functions $v_\\pi(s)$ can be non-stationary&lt;/li&gt;\n  &lt;li&gt;Feedback is delayed, not instantaneous&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;classes-of-function-approximation&quot;&gt;Classes of Function Approximation&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;Tabular: a table with an entry for each MDP state&lt;/li&gt;\n  &lt;li&gt;State aggregation: Partition environment states&lt;/li&gt;\n  &lt;li&gt;Linear function approximate: fixed feature (or fixed kernel)&lt;/li&gt;\n  &lt;li&gt;Differentiable (nonlinear) function approximation: neural nets&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;approximate-values-by-stochastic-gradient-descent&quot;&gt;Approximate Values By Stochastic Gradient Descent&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;Goal: fins $\\theta$ that minimise the difference between $v_\\theta(s)$ and   $v_\\pi(s)$ \n&lt;script type=&quot;math/tex&quot;&gt;J(\\theta)=\\mathbb{E}[(v_\\pi(S)-v_\\theta(S))^2]&lt;/script&gt;\n      Note: The expectation if over the state distribution — e.g., induced by the policy&lt;/li&gt;\n  &lt;li&gt;Gradient descent:&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\Delta\\theta=-\\frac{1}{2}\\alpha\\nabla_\\theta J(\\theta)=\\alpha\\mathbb{E}[(v_\\pi(S)-v_\\theta(S))\\nabla_\\theta v_\\theta(S)]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Stochastic&lt;/strong&gt; gradient descent&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\Delta\\theta_t=\\alpha(v_\\pi(S_t)-v_\\theta(S_t))\\nabla_\\theta v_\\theta(S_t)&lt;/script&gt;\n    &lt;h3 id=&quot;feature-vectors&quot;&gt;Feature Vectors&lt;/h3&gt;\n    &lt;hr /&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Represent state by a &lt;strong&gt;feature vector&lt;/strong&gt; &lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\phi(s) = \\left(\\begin{array}{cc}\n\\phi_1(s) \\\\ \\vdots \\\\ \\phi_n(s)\n\\end{array}\\right)&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;$\\phi:S\\rightarrow\\mathbb{R}^n$ is a fixed mapping from state (e.g. observation) to features&lt;/li&gt;\n  &lt;li&gt;Short-hand: $\\phi_t=\\phi(S_t)$&lt;/li&gt;\n  &lt;li&gt;For example:\n    &lt;ul&gt;\n      &lt;li&gt;Distance of robot from landmarks&lt;/li&gt;\n      &lt;li&gt;Trends in the stock market&lt;/li&gt;\n      &lt;li&gt;Piece and pawn configurations in chess&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;approximate-values-by-stochastic-gradient-descent-1&quot;&gt;Approximate Values By Stochastic Gradient Descent&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Goal: fina $\\theta$ that minimise the difference between $v_\\theta(s)$ and $v_\\pi(s)$ \n&lt;script type=&quot;math/tex&quot;&gt;J(\\theta)=\\mathbb{E}[(v_\\pi(S)-v_\\theta(S))^2]&lt;/script&gt;  &lt;br /&gt;\nNote: The expectation if over the state distribution — e.g., induced by the policy.&lt;/li&gt;\n  &lt;li&gt;Gradient descent:&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\Delta\\theta=-\\frac{1}{2}\\alpha\\Delta_\\theta J(\\theta)=\\alpha\\mathbb{E}_\\pi p[(v_\\pi(S)-v_\\theta(S))\\nabla_\\theta v_\\theta(S)]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;strong&gt;Stochastic&lt;/strong&gt; gradient descent:&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\Delta\\theta_t=\\alpha(v_\\pi(S_t)-v_\\theta(S_t))\\nabla_\\theta v_\\theta(S_t)&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;linear-value-function-approximation&quot;&gt;Linear Value Function Approximation&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Approximate value function by a linear combination of features&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;v_\\theta(s)=\\theta^\\top \\phi(s)=\\sum_{j=1}^n\\phi_j(s)\\theta_j&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Objective function (‘loss’) is quadratic in $\\theta$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;J(\\theta)=\\mathbb{E}_\\pi\\big[(v_\\pi(S)-\\theta^\\top\\phi(S))^2\\big]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Stichastic gradient descent converges on global ooptimum&lt;/li&gt;\n  &lt;li&gt;Update rule is simple&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\nabla_\\theta v_\\theta(S_t)=\\phi(S_t)=\\phi_t \\quad \\Longrightarrow \\quad \\Delta_\\theta = \\alpha(v_\\pi(S_t)-v_\\theta(S_t))\\phi_t&lt;/script&gt;\n      $\\large \\text{Update}=\\textbf{step size}\\times\\textbf{prediction error}\\times\\textbf{feature vector}$&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;incremental-prediction-algorithms&quot;&gt;Incremental Prediction Algorithms&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;The true value function $v_\\pi(s)$ is typically not available&lt;/li&gt;\n  &lt;li&gt;In practice, we substitute a &lt;strong&gt;target&lt;/strong&gt; for $v_\\pi(s)$\n    &lt;ul&gt;\n      &lt;li&gt;For MC, the target is the return $G_t$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\Delta\\theta_t=\\alpha(G_t - v_\\theta(s))\\nabla_\\theta v_\\theta(s)&lt;/script&gt;&lt;/li&gt;\n      &lt;li&gt;For TD, the target is the TD target $R_{t+1}+\\gamma v_\\theta(S_{t+1})$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\Delta\\theta_t=\\alpha(R_{t+1}+\\gamma v_\\theta(S_{t+1}) - v_\\theta(S_t))\\nabla_\\theta v_\\theta(S_t)&lt;/script&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;monte-carlo-with-value-function-approximation&quot;&gt;Monte-Carlo with Value Function Approximation&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;The return $G_t$ is an unbiased, noisy sample of $v_\\pi(s)$&lt;/li&gt;\n  &lt;li&gt;Can therefore apply supervised learning to (online) “training data”&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;{(S_0,G_0),\\ldots,(S_t,G_t)}&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;For example, using &lt;strong&gt;linear Monte-Carlo policy evaluation&lt;/strong&gt;&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n\\Delta\\theta_t &amp;=\\alpha(G_t - v_\\theta(S_t))\\nabla_\\theta v_\\theta(S_t) \\\\\n&amp; = \\alpha(G_t - v_\\theta(S_t))\\phi_t\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Monte-Carlo evaluation converges to a local optimum&lt;/li&gt;\n  &lt;li&gt;Even when using non-linear value function approximation&lt;/li&gt;\n  &lt;li&gt;For linear function, it finds the globlal optimum&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;td-learning-with-value-function-approximation&quot;&gt;TD Learning with Value Function Approximation&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;The TD-target $R_{t+1}+\\gamma v_\\theta(S_{t+1})$ is a &lt;strong&gt;biased&lt;/strong&gt; sample og true value $v_\\pi(S_t)$&lt;/li&gt;\n  &lt;li&gt;Can still apply supervised learning to “training data”&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;{(S_0,R_1+\\gamma v_\\theta(S_1)),\\ldots,(S_t,R_{t+1}+\\gamma v_\\theta(S_{t+1}))}&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;For example, using linear TD&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n\\Delta\\theta_t &amp;= \\alpha\\underbrace{(R_{t+1}+\\gamma v_\\theta(S_{t+1})-v_\\theta(S_t))}_{\\normalsize =\\delta_t, \\text{TD error}}\\nabla_\\theta v_\\theta(S_t) \\\\\n&amp; =\\alpha\\delta_t\\phi_t\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;convergence-of-mc-and-td&quot;&gt;Convergence of MC and TD&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;\n    &lt;p&gt;with linear functions, MC converges to&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\min_\\theta\\mathbb{E}\\big[(G_t-v_\\theta(S_t))^2\\big]=\\mathbb{E}\\big[\\phi_t\\phi_t^\\top\\big]^{-1}\\mathbb{E}\\big[v_\\pi(S_t)\\phi_t\\big]&lt;/script&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;With linear function, TD converges to&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\min_\\theta\\mathbb{E}\\big[(R_{t+1}+\\gamma v_\\theta(S_{t+1}-v_\\theta(S_t)))^2\\big]=\\mathbb{E}\\big[\\phi_t(\\phi_t-\\gamma\\phi_{t+1})^\\top\\big]\\mathbb{E}\\big[R_{t+1}\\phi_t\\big]&lt;/script&gt;\n(in continuing problem with fixed $\\gamma$)&lt;/li&gt;\n  &lt;li&gt;This is a different solution from MC&lt;/li&gt;\n  &lt;li&gt;Typically, the asymptotic MC solution is preferred&lt;/li&gt;\n  &lt;li&gt;But TD methods may converge faster,   and may still be better&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\textbf{TD:}\\quad\\Delta_t=\\alpha\\delta\\nabla_\\theta v_\\theta(S_t)\\quad \\textbf{where} \\quad \\delta_t=R_{t+1}+\\gamma v_\\theta(S_{t+1}-v_\\theta(S_t))&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;This update ignores dependence of $v_\\theta(S_{t+1})$ on $\\theta$&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;action-value-function-approximation&quot;&gt;Action-Value Function Approximation&lt;/h3&gt;\n&lt;ul&gt;\n  &lt;li&gt;Approximate the action-value function&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;q_\\theta(s,a)\\approx q_\\pi(S,a)&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;For instance, with linear function approximation&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;q_\\theta(s,a)=\\phi(s,a)_\\top\\theta=\\sum_{j=1}^n\\phi_j(s,a)\\theta_j&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Stochastic gradient descent update&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n\\Delta\\theta &amp;= \\alpha(q_\\pi(s,a)-q_\\theta(s,a))\\nabla_\\theta q_\\theta(s,a) \\\\\n&amp;= \\alpha(q_\\pi(s,a)-q_\\theta(s,a))\\phi(s,a)\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;least-squarse-prediction&quot;&gt;Least Squarse Prediction&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;Given value function approximation $v_\\theta(s) \\approx v_\\pi(s)$&lt;/li&gt;\n  &lt;li&gt;And &lt;strong&gt;experience&lt;/strong&gt; $\\mathcal{D}$ consisting of $\\langle \\text{state, estimated value} \\rangle$pairs&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\mathcal{D}=\\big\\{\\langle S_1,\\hat{v}_1^\\pi \\rangle,\\langle S_2,\\hat{v}_2^\\pi \\rangle,\\ldots,\\langle S_T,\\hat{v}_T^\\pi \\rangle \\big\\}&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;E.g., $\\hat{V}_1^\\pi=R_{t+1}+\\gamma v_\\theta(S_{t+1})$&lt;/li&gt;\n  &lt;li&gt;Which parameters $\\theta$ give the best fitting value function $v_\\theta(s)$?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;stochastic-gradient-descent-with-experience-replay&quot;&gt;Stochastic Gradient Descent with Experience Replay&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;p&gt;Give experience consisting of $\\large \\langle \\text{state, value} \\rangle$pairs&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\mathcal{D}=\\big\\{\\langle S_1,\\hat{v}_1^\\pi \\rangle,\\langle S_2,\\hat{v}_2^\\pi \\rangle,\\ldots,\\langle S_T,\\hat{v}_T^\\pi \\rangle \\big\\}&lt;/script&gt;&lt;br /&gt;\nRepeat:&lt;/p&gt;\n&lt;ol&gt;\n  &lt;li&gt;Sample state, value from experience&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\langle s, \\hat{v}_\\pi  \\rangle \\sim \\mathcal{D}&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Apply stochastic gradient decent update&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\Delta\\theta = \\alpha(\\hat{v}^\\pi - v_\\theta(s))\\nabla_\\theta v_\\theta(s)&lt;/script&gt;&lt;br /&gt;\nConverges to least squares solution&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\theta_\\pi=\\mathop{\\text{argmin}}\\limits_\\theta LS(\\theta)=\\mathop{\\text{argmin}}\\limits_\\theta\\mathbb{E}_\\mathcal{D}\\big[(\\hat{v}_i^\\pi-v_\\theta(S_i))^2\\big]&lt;/script&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h3 id=&quot;linear-least-squares-prediction&quot;&gt;Linear Least Squares Prediction&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;Experience replay finds least squares solution&lt;/li&gt;\n  &lt;li&gt;But it may take many iterations&lt;/li&gt;\n  &lt;li&gt;Using &lt;strong&gt;linear&lt;/strong&gt; value function approximation $v_\\theta(s)=\\phi(s)^\\top\\theta$ we can solve the least squares solution directly&lt;/li&gt;\n  &lt;li&gt;At minimum of $LS(\\theta)$, the expected update must be zero&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n\\mathbb{E}_\\mathcal{D}[\\Delta\\theta] &amp;= 0 \\\\\n\\alpha\\sum_{t=1}^T\\phi_t(\\hat{v}_t^\\pi-\\phi_t^\\top\\theta) &amp;= 0 \\\\\n\\sum_{t=1}^T\\phi_t\\hat{v}_t^\\pi &amp;= \\sum_{t=1}^T\\phi_t\\phi_t^\\top\\theta \\\\\n\\theta_t &amp;= \\Big(\\sum_{t=1}^T\\phi_t\\phi_t^\\top\\Big)^{-1}\\sum_{t=1}^T\\phi_t\\hat{v}_t^\\pi\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;For N feature, direct solution time is $O(N^3)$&lt;/li&gt;\n  &lt;li&gt;Incremental solution time is $O(N^2)$ using Shermann-Morrison&lt;/li&gt;\n  &lt;li&gt;We do not know true values $v_\\pi$ (have estimates $\\hat{v}_t$)&lt;/li&gt;\n  &lt;li&gt;In practice, our “training data” must use noisy or biased sample of $v_\\pi$&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;      &amp;lt;font color=blue&amp;gt;&lt;strong&gt;LSMC&lt;/strong&gt;&amp;lt;/font&amp;gt; Least Squares Monte-Carlo uses return&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;v_\\pi \\approx G_t&lt;/script&gt;\n      &amp;lt;font color=blue&amp;gt;&lt;strong&gt;LSTD&lt;/strong&gt;&amp;lt;/font&amp;gt; Least Squares Temporal-Difference uses TD target&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;v_\\pi \\approx R_{t+1} + \\gamma v_\\theta(S_{t+1})&lt;/script&gt;&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;In each case we can solve directly for the fixed poine&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;deep-reinforcement-learning&quot;&gt;Deep reinforcement learning&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;Many ideas immediately transfer when using deep neural networks:\n    &lt;ul&gt;\n      &lt;li&gt;TD and MC&lt;/li&gt;\n      &lt;li&gt;Double learning (e.g., double Q-learning)&lt;/li&gt;\n      &lt;li&gt;Experience replay&lt;/li&gt;\n      &lt;li&gt;…&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Some ideas do not easily transfer\n    &lt;ul&gt;\n      &lt;li&gt;UCB&lt;/li&gt;\n      &lt;li&gt;Least squares TD/MC&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;neural-q-learning&quot;&gt;Neural Q-learning&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;Online neural Q-learning may include:\n    &lt;ul&gt;\n      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $q_\\theta:\\space O_t \\Longrightarrow (q[1],\\ldots,q[m])(m\\space \\text{actions})$&lt;/li&gt;\n      &lt;li&gt;An $\\epsilon-\\text{greedy}$ &lt;strong&gt;exploration policy&lt;/strong&gt;: $q_t\\space \\Longrightarrow \\space \\pi_t \\Longrightarrow \\space A_t$&lt;/li&gt;\n      &lt;li&gt;A Q-learning &lt;strong&gt;loss function&lt;/strong&gt; on $\\theta$&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;I(\\theta)=\\frac{1}{2}\\Big(R_{t+1}+\\gamma\\Big[\\max_a q_\\theta (S_{t+1},a)\\Big] - q_\\theta (S_t, A_t)\\Big)^2&lt;/script&gt;&lt;br /&gt;\nwhere $[\\cdot ]$ denotes stopping the gradient, so that the gradient is&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;\\nabla_\\theta I(\\theta)=\\Big(R_{t+1}+\\gamma\\max_a q_\\theta(S_{t+1},a)-q_\\theta(S_t,A_t)\\Big)\\nabla_\\theta q_\\theta(S_t,A_t)&lt;/script&gt;&lt;/li&gt;\n      &lt;li&gt;An &lt;strong&gt;optimizer&lt;/strong&gt; to minimize the loss (e.g., SGD, RMSProp, Adma)&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;dqn&quot;&gt;DQN&lt;/h3&gt;\n&lt;hr /&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;DQN (Mnih et al. 2013, 2015) includes;\n    &lt;ul&gt;\n      &lt;li&gt;A &lt;strong&gt;network&lt;/strong&gt; $q_\\theta:\\space O_t \\mapsto (q[1],\\ldots,q[m])(m\\space \\text{actions})$&lt;/li&gt;\n      &lt;li&gt;An $\\epsilon-\\text{greedy}$ &lt;strong&gt;exploration policy&lt;/strong&gt;: $q_t\\space \\mapsto \\space \\pi_t \\Longrightarrow \\space A_t$&lt;/li&gt;\n      &lt;li&gt;A &lt;strong&gt;replay buffer&lt;/strong&gt; to store and sample past transitions&lt;/li&gt;\n      &lt;li&gt;A &lt;strong&gt;target network&lt;/strong&gt; $q_{\\theta^-}:\\space Q_t \\mapsto\\space(q^-[1],\\ldots,q^-[m])$&lt;/li&gt;\n      &lt;li&gt;A Q-learning &lt;strong&gt;loss function&lt;/strong&gt; on $\\theta$ (use replay and target network) &lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;I(\\theta)=\\frac{1}{2}\\Big(R_{t+1}+\\gamma\\Big[\\max_a q_{\\theta^-} (S_{t+1},a)\\Big] - q_\\theta (S_t, A_t)\\Big)^2&lt;/script&gt;&lt;/li&gt;\n      &lt;li&gt;An &lt;strong&gt;optimizer&lt;/strong&gt; to minimize the loss (e.g., SGD, RMSProp, Adma)&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Replay and target networks make RL look more like supervised learning&lt;/li&gt;\n  &lt;li&gt;It is unclear whether they are vital, but they help&lt;/li&gt;\n  &lt;li&gt;“DL-aware RL”&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;n-step-return&quot;&gt;n-Step Return&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;\n    &lt;p&gt;Consider the following n-steps returns for $n=1,2,\\infty: $&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n&amp;n=1 \\quad &amp;(TD)\\quad &amp; G_T^{(1)}=R_{t+1} + \\gamma v(S_{t+1}) \\\\\n&amp;n=2 \\quad &amp;\\quad &amp; G_T^{(2)}=R_{t+1} + \\gamma R_{t+2} +\\gamma^2 v(S_{t+2}) \\\\\n&amp;\\quad \\vdots \\quad &amp;\\quad &amp; \\vdots \\\\\n&amp;n=\\infty \\quad &amp;(MC)\\quad &amp; G_T^{(\\infty)}=R_{t+1} + \\gamma R_{t+2}+\\ldots+\\gamma^{T-t-1}R_T\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Define the n-step return&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n v(S_{t+n})&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;n-step temporal-difference learning&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;v(S_t) \\leftarrow v(S_t) + \\alpha\\Big(G_t^{(n)} - v(S_t)\\Big)&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n        Sun, 12 May 2019 00:00:00 -0400\n        http://localhost:4000/2019/Function-Approximation/\n        http://localhost:4000/2019/Function-Approximation/\n        \n        \n      \n    \n      \n        Markov descision processess\n        &lt;h3 id=&quot;introduction-to-mdps&quot;&gt;Introduction to MDPs&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ol&gt;\n  &lt;li&gt;&lt;strong&gt;Markov decision processes (MDPs)&lt;/strong&gt; formally describe an environment.&lt;/li&gt;\n  &lt;li&gt;Assume the environment is fully observable the current: the current observations contains relevant information.&lt;/li&gt;\n  &lt;li&gt;Almost all RL problems can be formalized as MDPs, e.g,\n    &lt;ul&gt;\n      &lt;li&gt;Optimal control primarily deals with continuous MDPs&lt;/li&gt;\n      &lt;li&gt;Partially observable problems can be converted into MDPs&lt;/li&gt;\n      &lt;li&gt;Bandits are MDPs with one state&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;!--more--&gt;\n&lt;hr /&gt;\n&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;\n&lt;p&gt;A state $s$ has the &lt;strong&gt;Markvo&lt;/strong&gt; property when for states $\\forall s^{‘}\\in S$ and all rewards $r \\in \\mathbb{R}$ \n&lt;script type=&quot;math/tex&quot;&gt;p(R_{t+1}=r,S_{t+1}=s^{'}|S_{t}=s)= \np(R_{t+1}=r,S_{t+1}=s^{'}|S_{1},...,S_{t},S_{t-1}=s)&lt;/script&gt;&lt;br /&gt;\nfor all possible histories $S_{1},…,S_{t-1}$&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;The state capture all relevent information from history&lt;/li&gt;\n  &lt;li&gt;Once the state is know, the history may be throw away&lt;/li&gt;\n  &lt;li&gt;The state is a sufficient statistic of the past&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;return&quot;&gt;Return&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;\n    &lt;p&gt;Acting in a MDP results in &lt;strong&gt;return&lt;/strong&gt; $G_{t}$: total discounted reward from time-step $t$&lt;br /&gt;\n  &lt;script type=&quot;math/tex&quot;&gt;G_{t}=R_{t+1}+\\gamma R_{t+2}+ ... = \\sum_{k=0}^{\\infty}\\gamma_{k}R_{t+k+1}&lt;/script&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;This is a random variables that depends on &lt;strong&gt;MDP&lt;/strong&gt; and &lt;strong&gt;policy&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;The &lt;strong&gt;discount&lt;/strong&gt; $\\gamma\\in[0,1]$ is the present value of future rewards\n    &lt;ul&gt;\n      &lt;li&gt;The marginal value of receiving reward $R$ after $k+1$time-steps is $\\gamma^{k}R$&lt;/li&gt;\n      &lt;li&gt;For $\\gamma&amp;lt;1$, immediate rewards are more important than delayed rewards&lt;/li&gt;\n      &lt;li&gt;$\\gamma$ close to 0 leads to “myopic” evaluation&lt;/li&gt;\n      &lt;li&gt;$\\gamma$ close to 1 leads to “far-sighted” evaluation&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;value-function&quot;&gt;Value Function&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;The value function $v(s)$ gives the long-term value of state $s$ &lt;br /&gt;\n  &lt;script type=&quot;math/tex&quot;&gt;v_{\\pi}(s)=\\mathbb{E}[G_{t}|S_{t}=s,\\pi]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;It can be defined recursively&lt;br /&gt;\n  &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n  v_{\\pi}(s) &amp;=\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|S_{t}, \\pi] \\\\\n  &amp;=\\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_{t}=s, A_{t}\\sim\\pi(S_{t})] \\\\\n  &amp;=\\sum_{a}\\pi(a|s)\\sum_{r}\\sum_{s^{'}}p(r,s^{'}|s,a)(r+\\gamma v_{\\pi}(s^{'}))\n  \\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;The final step writes out the expectation explicitly&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3 id=&quot;action-values&quot;&gt;Action Values&lt;/h3&gt;\n&lt;hr /&gt;\n&lt;ul&gt;\n  &lt;li&gt;We can define state-action values  &lt;br /&gt;\n  &lt;script type=&quot;math/tex&quot;&gt;q_{\\pi}(s,a)=\\mathbb{E}[G_{t}|S_{t}=s,A_{t}=a,\\pi]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;This implies: the value of a state is equal to the weighted sum of the state action value by definition&lt;br /&gt;\n  &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n  q_{\\pi}(s,a)&amp;=\\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_{t}=s, A_{t}=a] \\\\\n  &amp;=\\mathbb{E}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a] \\\\\n  &amp;=\\sum_{r}\\sum_{s^{'}}p(r,s^{'}|s,a)\\Big(r+\\gamma\\sum_{a^{'}}\\pi(a^{'},s^{'})q_{\\pi}(s^{'},a^{'})\\Big)\n  \\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;Note that &lt;br /&gt;\n  &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\n  v_{\\pi}(s) &amp; =  \\sum_{a}\\pi(a|s)q_{\\pi}(s,a) \\\\ \n  &amp; = \\mathbb{E}[q_{\\pi}(S_{t},A_{t})|S_{t}=s,\\pi], \\forall s\n  \\end{aligned} %]]&gt;&lt;/script&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h4 id=&quot;tips&quot;&gt;Tips&lt;/h4&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;Estimating $v_{\\pi}$ or $q_{\\pi}$ is called &lt;strong&gt;policy evaluation&lt;/strong&gt; or, simply, &lt;strong&gt;prediction&lt;/strong&gt;&lt;/li&gt;\n  &lt;li&gt;Estimating $v_{\\star}\\space \\text{or}\\space q_{\\star}$ is sometimes called &lt;strong&gt;control&lt;/strong&gt;, because these can be used for &lt;strong&gt;policy optimizaton&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr /&gt;\n&lt;h3 id=&quot;bellman-equation&quot;&gt;Bellman Equation&lt;/h3&gt;\n&lt;p&gt;Four Bellman equations:&lt;br /&gt;\n&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[\n\\begin{aligned}\nv_{\\pi}(s) &amp;=\\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_{t}=s,A_{t}\\sim\\pi(S_{t})] \\\\\nv_{*}(s) &amp;=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v_{*}(S_{t+1})|S_{t}=s,A_{t}=a] \\\\\nq_{\\pi}(s,a) &amp;=\\mathbb{E}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1}, A_{t+1})|S_{t}=s,A_{t}=a] \\\\\nq_{*}(s,a) &amp;=\\mathbb{E}[R_{t+1}+\\gamma \\max_{a^{'}} q_{*}(S_{t+1},a^{'})|S_{t}=s,A_{t}=a]\n\\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;\n\n&lt;hr /&gt;\n&lt;p&gt;### Policy Evaluate&lt;/p&gt;\n\n&lt;ul&gt;\n  &lt;li&gt;We start by discussing how to estimate&lt;br /&gt;\n &lt;script type=&quot;math/tex&quot;&gt;v_{\\pi}(s)=\\mathbb[R_{t+1}=\\gamma v_{\\pi}(S_{t+1})|s,\\pi]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;Idea: turn this equality into an update&lt;/li&gt;\n  &lt;li&gt;First, initialize $v_{0}$ e.g. to zero.&lt;/li&gt;\n  &lt;li&gt;Then iterate&lt;br /&gt;\n &lt;script type=&quot;math/tex&quot;&gt;\\forall s: v_{k+1}(s)=\\mathbb{E}[R_{t+1}+\\gamma v_{k}(S_{t+1})|s,\\pi]&lt;/script&gt;&lt;/li&gt;\n  &lt;li&gt;\n    &lt;p&gt;Note: whenever $v_{k+1}(s)=v_{k}(s)$, for all $s$, we must have found $v_{\\pi}$&lt;/p&gt;\n  &lt;/li&gt;\n  &lt;li&gt;This policy evaluation is always converge under appropriate conditions (e.g., $\\gamma &amp;lt; 1$)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Implies $\\lim_{k\\rightarrow\\infty}v_{k}=v_{\\pi}$&lt;/p&gt;\n&lt;ul&gt;\n  &lt;li&gt;Finite-horizon episodic case is a bit harder, but also works&lt;/li&gt;\n&lt;/ul&gt;\n\n\n        Fri, 03 May 2019 00:00:00 -0400\n        http://localhost:4000/2019/Markov-Descision-Processess/\n        http://localhost:4000/2019/Markov-Descision-Processess/"
					}
					
				
		
				
					,
					
					"": {
						"id": "",
						"title": "",
						"categories": "",
						"url": " /",
						"content": "Planning and models\n   May 23, 2019\n   \n   1. Model-Based and Model-Free RL\n\n  Model-Free RL\n    \n      No model\n      Learn value function (and/or policy) from experience\n    \n  \n  Model-Based RL\n    \n      Learn a model from experience OR be given a model\n      Plan value function (and/or policy) from model\n    \n  \n\n\n\n   \n      read more\n   \n   \n\n\n\n   Policy gradients and actor critic\n   May 17, 2019\n   \n   General Overview\n\n  Model-based RL:\n + ‘Easy’ to learn a model (supervised learning) \n + Learns ‘all there is to know’ from the data\n - Objective capture irrelevant information\n - May focus compute/capacity on irrelevant detail\n - Computing policy (planning) is non-trivial and can be computationally expensive\n  Valued-based RL:\n + Closer to true objective\n + Fairly well-understood — somewhat similar to regression\n - Still not the true objective — may still focus capacity on less-important details\n  Policy-based RL:\n + Right objective!\n - Ignores other learnable knowledge (potentially not the most efficient use of data)\n\n\n\n   \n      read more\n   \n   \n\n\n\n   Function approximation\n   May 12, 2019\n   \n   Introduce\n\n\n  The policy, value function and model are all functions\n  We want to learn (one of) these from experience\n  If there are too many states, we need to approximate\n  In general, this is called RL with function approximation\n  When using deep neural nets, this is often called deep reinforcement learning\n  The term is fairly new — the combination is decades old\n\n\n\n   \n      read more\n   \n   \n\n\n\n   Markov descision processess\n   May 03, 2019\n   \n   Introduction to MDPs\n\n\n  Markov decision processes (MDPs) formally describe an environment.\n  Assume the environment is fully observable the current: the current observations contains relevant information.\n  Almost all RL problems can be formalized as MDPs, e.g,\n    \n      Optimal control primarily deals with continuous MDPs\n      Partially observable problems can be converted into MDPs\n      Bandits are MDPs with one state\n    \n  \n\n\n\n   \n      read more"
					}
					
				
		
				
		
				
					,
					
					"assets-js-main-js": {
						"id": "assets-js-main-js",
						"title": "",
						"categories": "",
						"url": " /assets/js/main.js",
						"content": "(function($) {\n    'use strict';\n    $(function() {\n        $('[data-toggle=\"tooltip\"]').tooltip();\n        $('[data-toggle=\"popover\"]').popover();\n        $('.popover-dismiss').popover({\n            trigger: 'focus'\n        })\n    });\n\n    function bottomPos(element) {\n        return element.offset().top + element.outerHeight();\n    }\n    $(function() {\n        var promo = $(\".js-td-cover\");\n        if (!promo.length) {\n            return\n        }\n        var promoOffset = bottomPos(promo);\n        var navbarOffset = $('.js-navbar-scroll').offset().top;\n        var threshold = Math.ceil($('.js-navbar-scroll').outerHeight());\n        if ((promoOffset - navbarOffset) < threshold) {\n            $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n        }\n        $(window).on('scroll', function() {\n            var navtop = $('.js-navbar-scroll').offset().top - $(window).scrollTop();\n            var promoOffset = bottomPos($('.js-td-cover'));\n            var navbarOffset = $('.js-navbar-scroll').offset().top;\n            if ((promoOffset - navbarOffset) < threshold) {\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n            } else {\n                $('.js-navbar-scroll').removeClass('navbar-bg-onscroll');\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll--fade');\n            }\n        });\n    });\n}(jQuery));\n(function($) {\n    'use strict';\n    var Search = {\n        init: function() {\n            $(document).ready(function() {\n                $(document).on('keypress', '.td-search-input', function(e) {\n                    if (e.keyCode !== 13) {\n                        return\n                    }\n                    var query = $(this).val();\n                    var searchPage = \"http://localhost:4000/search/?q=\" + query;\n                    document.location = searchPage;\n                    return false;\n                });\n            });\n        },\n    };\n    Search.init();\n}(jQuery));"
					}
					
				
		
				
					,
					
					"news": {
						"id": "news",
						"title": "Latest articles",
						"categories": "",
						"url": " /news/",
						"content": "Latest articles\n\n\n\n\n\n\n   Planning and models\n   May 23, 2019\n   \n   1. Model-Based and Model-Free RL\n\n  Model-Free RL\n    \n      No model\n      Learn value function (and/or policy) from experience\n    \n  \n  Model-Based RL\n    \n      Learn a model from experience OR be given a model\n      Plan value function (and/or policy) from model\n    \n  \n\n\n\n   \n      read more\n   \n   \n\n\n\n   Policy gradients and actor critic\n   May 17, 2019\n   \n   General Overview\n\n  Model-based RL:\n + ‘Easy’ to learn a model (supervised learning) \n + Learns ‘all there is to know’ from the data\n - Objective capture irrelevant information\n - May focus compute/capacity on irrelevant detail\n - Computing policy (planning) is non-trivial and can be computationally expensive\n  Valued-based RL:\n + Closer to true objective\n + Fairly well-understood — somewhat similar to regression\n - Still not the true objective — may still focus capacity on less-important details\n  Policy-based RL:\n + Right objective!\n - Ignores other learnable knowledge (potentially not the most efficient use of data)\n\n\n\n   \n      read more\n   \n   \n\n\n\n   Function approximation\n   May 12, 2019\n   \n   Introduce\n\n\n  The policy, value function and model are all functions\n  We want to learn (one of) these from experience\n  If there are too many states, we need to approximate\n  In general, this is called RL with function approximation\n  When using deep neural nets, this is often called deep reinforcement learning\n  The term is fairly new — the combination is decades old\n\n\n\n   \n      read more\n   \n   \n\n\n\n   Markov descision processess\n   May 03, 2019\n   \n   Introduction to MDPs\n\n\n  Markov decision processes (MDPs) formally describe an environment.\n  Assume the environment is fully observable the current: the current observations contains relevant information.\n  Almost all RL problems can be formalized as MDPs, e.g,\n    \n      Optimal control primarily deals with continuous MDPs\n      Partially observable problems can be converted into MDPs\n      Bandits are MDPs with one state\n    \n  \n\n\n\n   \n      read more\n   \n   \n\n\nWant to see more? See the Articles."
					}
					
				
		
				
		
				
		
				
					,
					
					"sitemap-xml": {
						"id": "sitemap-xml",
						"title": "",
						"categories": "",
						"url": " /sitemap.xml",
						"content": "/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% for section in site.data.toc %}\n     {{ site.baseurl }}{{ section.url }}/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% endfor %}"
					}
					
				
		
				
					,
					
					"tags": {
						"id": "tags",
						"title": "Tags Index",
						"categories": "",
						"url": " /tags/",
						"content": "Tags Index\n{% capture site_tags %}{% for tag in site.tags %}{% if tag %}{{ tag | first }}{% unless forloop.last %},{% endunless %}{% endif %}{% endfor %}{% endcapture %}{% assign docs_tags = \"\" %}{% for doc in site.docs %}{% assign ttags = doc.tags | join:',' | append:',' %}{% assign docs_tags = docs_tags | append:ttags %}{% endfor %}\n{% assign all_tags = site_tags | append:docs_tags %}{% assign tags_list = all_tags | split:',' | uniq | sort %}\n\n{% for tag in tags_list %}{% if tag %}{{ tag }}\n\n    {% for post in site.tags[tag] %}\n    {{- post.title -}}\n     {{- post.date | date: \"%B %d, %Y\" -}}\n{%- endfor -%}\n{% for doc in site.docs %}{% if doc.tags contains tag %}\n\n    {{ doc.title }}\n         {{- doc.date | date: \"%B %d, %Y\" -}}\n    {% endif %}{% endfor %}\n{% endif %}{%- endfor -%}"
					}
					
				
		
				
					,
					
					"assets-css-style-css": {
						"id": "assets-css-style-css",
						"title": "",
						"categories": "",
						"url": " /assets/css/style.css",
						"content": "@import \"jekyll-theme-primer\";"
					}
					
				
		
	};
</script>
<script src="/assets/js/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>

<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

<script>
$(document).ready(function() {

    var toc = $('#TOC');

    // Select each header
    sections = $('.td-content h1');
        $.each(sections, function(idx, v) {
            section = $(v);
            var div_id = $(section).attr('id');
            var div_text = section.text().split('¶')[0];
            var parent = $("#" + div_id)
            var content = '<li id="link_' + div_id + '" class="md-nav__item"><a class="md-nav__link" href="#' + div_id + '" title="' + div_text +'">' + div_text +'</a></li>';
            $(toc).append(content);

            // Add section code to subnavigation
            var children = $('<nav class="md-nav"><ul class="md-nav__list"></nav></ul>')
            var contenders = $("#" + div_id).nextUntil("h1");
            $.each(contenders, function(idx, contender){
               if($(contender).is('h2') || $(contender).is('h3')) {
                   var contender_id = $(contender).attr('id');
                   var contender_text = $(contender).text().split('¶')[0];
                   var content = '<li class="md-nav__item"><a class="md-nav__link" href="#' + contender_id + '" title="' + contender_text +'">' + contender_text +'</a></li>';
                   children.append(content);
                }
             })
             $("#link_" + div_id).append(children);
        });
    });
</script>

<script>
var headers = ["h1", "h2", "h3", "h4"]
var colors = ["red", "orange", "green", "blue"]

$.each(headers, function(i, header){
    var color = colors[i];
    $(header).each(function () {
        var href=$(this).attr("id");
        $(this).append('<a class="headerlink" style="color:' + color + '" href="#' + href + '" title="Permanent link">¶</a>')
    });
})
</script>



	
              
              <!-- <br/>

 -->
           </div>
          </main>
        </div>
      </div>
      <footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        

</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="GitHub" data-original-title="GitHub">
    <a class="text-white" target="_blank" href="https://github.com/wyk2796">
      <i class="fab fa-github"></i>
    </a>
  </li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
  <small class="text-white">© 2019 Yukai Wu All Rights Reserved</small>
  
  <p class="mt-2"><a href="/about/">About Docsy</a></p>	
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<script src="/assets/js/main.js"></script>

  </body>
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>  
</html>
